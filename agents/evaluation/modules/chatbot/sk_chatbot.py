import os
from typing import TypedDict, Literal, Optional, List, Dict
from datetime import datetime

from langchain_openai import ChatOpenAI
from langchain.embeddings import HuggingFaceEmbeddings
from pinecone import Pinecone
from sqlalchemy import create_engine, text
from langgraph.graph import StateGraph, END, START

from dotenv import load_dotenv
load_dotenv()

# ê¸°ì¡´ ì„¤ì • ê°€ì ¸ì˜¤ê¸° (ìƒëŒ€ ê²½ë¡œë¡œ ì‹œë„)
try:
    from config.settings import settings
except ImportError:
    # config ëª¨ë“ˆì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
    class DefaultSettings:
        DATABASE_URL = "mysql+pymysql://root:1234@localhost:3306/skoro_db"
    settings = DefaultSettings()

# =============================================================================
# ChatState ì •ì˜
# =============================================================================

class ChatState(TypedDict):
    # ì‚¬ìš©ì ì •ë³´
    user_id: str
    chat_mode: Literal["default", "appeal_to_manager"]
    user_input: str
    role: Literal["MANAGER", "MEMBER"]
    team_id: str
    appeal_complete: Optional[bool]
    
    # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤
    retrieved_docs: List[Dict]
    
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬
    qna_dialog_log: List[str]
    dialog_log: List[str]
    
    # í”Œë˜ê·¸ ë° ê²°ê³¼
    summary_draft: Optional[str]
    llm_response: Optional[str]

# =============================================================================
# ì±—ë´‡ ì„¤ì • (ì™„ì „í•œ .env ê¸°ë°˜)
# =============================================================================

class ChatbotConfig:
    def __init__(self):
        # .env íŒŒì¼ì—ì„œ í•„ìš”í•œ ê°’ë“¤ ë¡œë“œ
        self.pinecone_api_key = os.getenv("PINECONE_API_KEY")
        self.embedding_model_name = os.getenv("EMBEDDING_MODEL_NAME", "snunlp/KR-SBERT-V40K-klueNLI-augSTS")
        self.index_reports_name = os.getenv("PINECONE_INDEX_REPORTS", "skoro-chatbot")
        self.index_policy_name = os.getenv("PINECONE_INDEX_POLICY", "eval-policy-docs")
        self.index_appeals_name = os.getenv("PINECONE_INDEX_APPEALS", "appeal-cases")
        
        print(f"ğŸ”§ ì„¤ì • ë¡œë“œ ì¤‘...")
        print(f"  - Pinecone API Key: {'ì„¤ì •ë¨' if self.pinecone_api_key else 'ì—†ìŒ'}")
        print(f"  - ì„ë² ë”© ëª¨ë¸: {self.embedding_model_name}")
        print(f"  - ë¦¬í¬íŠ¸ ì¸ë±ìŠ¤: {self.index_reports_name}")
        
        # í•„ìˆ˜ê°’ ì²´í¬ (ê°œë°œìš©ìœ¼ë¡œ ì™„í™”)
        if not self.pinecone_api_key:
            print("âš ï¸ PINECONE_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.pinecone_api_key = "test-key"  # í…ŒìŠ¤íŠ¸ìš©
        
        try:
            # LLM ì„¤ì • (OPENAI_API_KEYëŠ” ChatOpenAIê°€ ìë™ìœ¼ë¡œ ì°¾ìŒ)
            self.llm = ChatOpenAI(model="gpt-4o", temperature=0)
            print("âœ… LLM ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            self.llm = None
        
        try:
            # ì„ë² ë”© ëª¨ë¸
            self.embedding_model = HuggingFaceEmbeddings(model_name=self.embedding_model_name)
            print("âœ… ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            self.embedding_model = None
        
        try:
            # Pinecone ì„¤ì •
            self.pc = Pinecone(api_key=self.pinecone_api_key)
            self.index_reports = self.pc.Index(self.index_reports_name)
            self.index_policy = self.pc.Index(self.index_policy_name)
            self.index_appeals = self.pc.Index(self.index_appeals_name)
            print("âœ… Pinecone ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ Pinecone ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            self.index_reports = None
            self.index_policy = None
            self.index_appeals = None
        
        try:
            # DB ì„¤ì •
            self.engine = create_engine(settings.DATABASE_URL)
            print("âœ… DB ì—°ê²° ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ DB ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            self.engine = None

# =============================================================================
# ì „ì—­ ê°ì²´ ì´ˆê¸°í™”
# =============================================================================

print("ğŸš€ ì±—ë´‡ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
config = ChatbotConfig()

# ì „ì—­ì—ì„œ ì‚¬ìš©í•  ê°ì²´ë“¤
llm = config.llm
embedding_model = config.embedding_model
index_reports = config.index_reports
index_policy = config.index_policy
index_appeals = config.index_appeals
engine = config.engine

# =============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =============================================================================

def get_user_metadata(user_id: str) -> dict:
    """ì‚¬ìš©ì ë©”íƒ€ë°ì´í„° ì¡°íšŒ"""
    if not engine:
        print("âš ï¸ DB ì—°ê²°ì´ ì—†ì–´ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return {"role": "MEMBER", "team_id": "default", "team_name": "default"}
    
    try:
        with engine.connect() as conn:
            query = text("""
                SELECT e.role, e.team_id, t.team_name
                FROM employees e
                JOIN teams t ON e.team_id = t.team_id
                WHERE e.emp_no = :user_id
            """)
            result = conn.execute(query, {"user_id": user_id}).fetchone()
            if result:
                return {
                    "role": result.role,
                    "team_id": result.team_id,
                    "team_name": result.team_name
                }
            else:
                return {"role": "MEMBER", "team_id": "default", "team_name": "default"}
    except Exception as e:
        print(f"âŒ ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return {"role": "MEMBER", "team_id": "default", "team_name": "default"}

# =============================================================================
# ì„¸ì…˜ ê´€ë¦¬ì
# =============================================================================

class SessionManager:
    """ë©”ëª¨ë¦¬ ê¸°ë°˜ ì„¸ì…˜ ê´€ë¦¬"""
    
    def __init__(self):
        self.sessions = {}
        print("âœ… ì„¸ì…˜ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get_session_state(self, user_id: str, chat_mode: str) -> Dict:
        session_key = f"{user_id}_{chat_mode}"
        return self.sessions.get(session_key, {})
    
    def save_session_state(self, user_id: str, chat_mode: str, state: Dict):
        session_key = f"{user_id}_{chat_mode}"
        self.sessions[session_key] = {
            "qna_dialog_log": state.get("qna_dialog_log", []),
            "dialog_log": state.get("dialog_log", []),
            "updated_at": datetime.now().isoformat()
        }
    
    def clear_session(self, user_id: str, chat_mode: str):
        session_key = f"{user_id}_{chat_mode}"
        if session_key in self.sessions:
            del self.sessions[session_key]

session_manager = SessionManager()

# =============================================================================
# LangGraph ë…¸ë“œë“¤
# =============================================================================

def initialize_state(state: ChatState) -> ChatState:
    """ìƒíƒœ ì´ˆê¸°í™”"""
    print(f"ğŸ”§ ìƒíƒœ ì´ˆê¸°í™”: {state['user_id']}")
    
    user_metadata = get_user_metadata(state["user_id"])
    
    state["role"] = user_metadata["role"]
    state["team_id"] = user_metadata["team_id"]
    
    if "retrieved_docs" not in state:
        state["retrieved_docs"] = []
    if "qna_dialog_log" not in state:
        state["qna_dialog_log"] = []
    if "dialog_log" not in state:
        state["dialog_log"] = []
    if "appeal_complete" not in state:
        state["appeal_complete"] = False
    
    return state

def route_chat_mode(state: ChatState) -> str:
    """ë¼ìš°íŒ…"""
    if state["chat_mode"] == "appeal_to_manager":
        if state.get("appeal_complete", False):
            return "summary_generator"
        else:
            return "appeal_dialogue"
    else:
        return "qna_agent"
    

def qna_agent_node(state: ChatState) -> ChatState:
    """QnA ì—ì´ì „íŠ¸ - ê°„ì†Œí™”ëœ ë²„ì „"""
    print("ğŸ’¬ QnA ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘...")
    
    query = state["user_input"]
    user_metadata = {
        "emp_no": state["user_id"],
        "role": state["role"],
        "team_name": state["team_id"]
    }

    # ì´ë²ˆ ì‚¬ìš©ì ë°œí™” ê¸°ë¡
    state["qna_dialog_log"].append(f"ì‚¬ìš©ì: {query}")

    # RAG ê²€ìƒ‰ì´ ê°€ëŠ¥í•œì§€ í™•ì¸
    if not (embedding_model and index_reports and index_policy and index_appeals):
        print("âš ï¸ RAG ê²€ìƒ‰ í™˜ê²½ì´ ì™„ì „í•˜ì§€ ì•Šì•„ ê¸°ë³¸ ì‘ë‹µì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        simple_response = f"í˜„ì¬ ì‹œìŠ¤í…œ ì ê²€ ì¤‘ì…ë‹ˆë‹¤. '{query}'ì— ëŒ€í•œ ìƒì„¸í•œ ë‹µë³€ì€ ì ì‹œ í›„ ì œê³µí•˜ê² ìŠµë‹ˆë‹¤."
        state["retrieved_docs"] = []
        state["llm_response"] = simple_response
        state["qna_dialog_log"].append(f"ì±—ë´‡: {simple_response}")
        return state

    try:
        # âœ… í†µí•© ê²€ìƒ‰ í•¨ìˆ˜ ì‚¬ìš© (ì¤‘ë³µ ì œê±°!)
        from rag_retriever import search_for_qna
        
        retrieved_docs = search_for_qna(query, user_metadata)

        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n\n".join(
            f"[ì¶œì²˜: {doc['source']}] {doc['content'][:500]}" for doc in retrieved_docs[:5]
        )

        # ì´ì „ ëŒ€í™”(ìµœê·¼ 10í„´) í¬í•¨í•˜ì—¬ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        conversation_context = "\n".join(state["qna_dialog_log"][-10:])

        # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if context.strip():
            prompt = f"""
ë‹¹ì‹ ì€ SKê·¸ë£¹ ì„±ê³¼í‰ê°€ ê¸°ì¤€ì— ê¸°ë°˜í•œ AI ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê³  ì‹ ë¢°ì„± ìˆê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì´ì „ ëŒ€í™”]
{conversation_context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

[ì°¸ê³  ë¬¸ì„œ]
{context}

[ì‘ë‹µ í˜•ì‹]
- ê°€ëŠ¥í•œ ê°ê´€ì  ê·¼ê±°ë¥¼ í¬í•¨í•˜ì—¬ ì„¤ëª…
- ì¶œì²˜(ë¦¬í¬íŠ¸, ì •ì±…, ì´ì˜ì œê¸° ë“±)ì— ë”°ë¼ ê·¼ê±° ë‹¤ë¥¼ ê²½ìš° ê°„ëµíˆ ì–¸ê¸‰
- ê¸°ì¤€ì´ ë¶ˆëª…í™•í•˜ë©´ "ì •ì±… ë¬¸ì„œì— ì •ì˜ë˜ì–´ ìˆì§€ ì•ŠìŒ"ì´ë¼ê³  ë§í•¨
- user_id(emp_no)ì— í•´ë‹¹í•˜ëŠ” ì§ì›, ì¦‰ ë³¸ì¸ ì™¸ì— ë‹¤ë¥¸ ì§ì›ì€ ìµëª…ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
        else:
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ
            prompt = f"""
ë‹¹ì‹ ì€ SKê·¸ë£¹ ì„±ê³¼í‰ê°€ ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¼ë°˜ì ì¸ ì„±ê³¼í‰ê°€ ê°€ì´ë“œë¼ì¸ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

[ì´ì „ ëŒ€í™”]
{conversation_context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{query}

ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¼ë°˜ì ì¸ ê°€ì´ë“œë¼ì¸ìœ¼ë¡œ ë‹µë³€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.
ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.
"""

        # LLM í˜¸ì¶œ
        if llm:
            print("ğŸ¤– LLM ì‘ë‹µ ìƒì„± ì¤‘...")
            try:
                llm_response = llm.predict(prompt)
                print(f"âœ… LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ ({len(llm_response)}ì)")
            except Exception as e:
                print(f"âš ï¸ LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
                llm_response = f"ì£„ì†¡í•©ë‹ˆë‹¤. '{query}'ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        else:
            llm_response = f"í˜„ì¬ ì‹œìŠ¤í…œ ì ê²€ ì¤‘ì…ë‹ˆë‹¤. '{query}'ì— ëŒ€í•œ ë‹µë³€ì€ ì ì‹œ í›„ ì œê³µí•˜ê² ìŠµë‹ˆë‹¤."

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        state["retrieved_docs"] = retrieved_docs
        state["llm_response"] = llm_response.strip()
        state["qna_dialog_log"].append(f"ì±—ë´‡: {llm_response.strip()}")

        print("âœ… QnA ì—ì´ì „íŠ¸ ì™„ë£Œ")

    except ImportError as e:
        print(f"âš ï¸ RAG ëª¨ë“ˆ import ì‹¤íŒ¨: {str(e)}")
        # ê¸°ë³¸ ì‘ë‹µ fallback (ê¸°ì¡´ê³¼ ë™ì¼)
        
    except Exception as e:
        print(f"âŒ QnA ì—ì´ì „íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        # ì—ëŸ¬ ì²˜ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼)

    return state   



def appeal_dialogue_node(state: ChatState) -> ChatState:
    """ì´ì˜ì œê¸° ëŒ€í™” ë…¸ë“œ - ê°„ì†Œí™”ëœ ë²„ì „"""
    print("ğŸ’¬ ì´ì˜ì œê¸° ëŒ€í™” ë…¸ë“œ ì‹¤í–‰ ì¤‘...")
    
    query = state["user_input"]
    user_metadata = {
        "emp_no": state["user_id"],
        "role": state["role"],
        "team_name": state["team_id"]
    }

    # í˜„ì¬ ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
    state["dialog_log"].append(f"ì‚¬ìš©ì: {query}")

    # íŒ©íŠ¸ ì²´í¬ í‚¤ì›Œë“œ ëª©ë¡
    FACT_TRIGGER_KEYWORDS = [
        "ì ìˆ˜", "ë“±ê¸‰", "ê¸°ì¤€", "í‰ê°€", "ìˆœìœ„", "ì»·ì˜¤í”„", "í‰ê°€í‘œ", "ì±„ì ", "ê·¼ê±°",
        "ê¸°ì—¬", "ë¹„ì¤‘", "ìˆ˜ì¹˜", "ë¹„ìœ¨", "ì •ëŸ‰", "ì •ì„±", "ì„±ê³¼", "í¼ì„¼íŠ¸", "%",
        "ê¸°ì¤€ì¹˜", "ì´ˆê³¼", "ë¯¸ë‹¬", "ë„ë‹¬", "ì¶©ì¡±", "ë‹¬ì„±", "ì¶©ë¶„", "ë¶€ì¡±",
        "ì–´ë””ì—", "ë¬¸ì„œ", "ê¸°ë¡", "ì •ì±…", "ê·œì •", "ì™œ", "ë¬´ì—‡ ë•Œë¬¸ì—", "ë¬´ìŠ¨ ì´ìœ "
    ]

    normalized_query = query.lower().replace(" ", "")
    needs_fact = any(k.replace(" ", "") in normalized_query for k in FACT_TRIGGER_KEYWORDS)

    # RAG ê²€ìƒ‰ì´ ê°€ëŠ¥í•œì§€ í™•ì¸
    if not (embedding_model and index_reports):
        print("âš ï¸ RAG ê²€ìƒ‰ í™˜ê²½ì´ ì™„ì „í•˜ì§€ ì•Šì•„ ê¸°ë³¸ ì‘ë‹µì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        simple_response = f"í˜„ì¬ ì‹œìŠ¤í…œ ì ê²€ ì¤‘ì…ë‹ˆë‹¤. '{query}'ì— ëŒ€í•œ ìƒì„¸í•œ ë‹µë³€ì€ ì ì‹œ í›„ ì œê³µí•˜ê² ìŠµë‹ˆë‹¤."
        state["retrieved_docs"] = []
        state["llm_response"] = simple_response
        state["dialog_log"].append(f"ì±—ë´‡: {simple_response}")
        return state

    try:
        # RAG ê²€ìƒ‰ (í•„ìš”í•  ë•Œë§Œ)
        retrieved_docs = []
        context = "(ì°¸ê³  ë¬¸ì„œ ì—†ìŒ)"
        
        if needs_fact:
            print("ğŸ” íŒ©íŠ¸ ì²´í¬ë¥¼ ìœ„í•œ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
            
            # âœ… í†µí•© ê²€ìƒ‰ í•¨ìˆ˜ ì‚¬ìš© (ì¤‘ë³µ ì œê±°!)
            from rag_retriever import search_for_appeal
            
            retrieved_docs = search_for_appeal(query, user_metadata)
            
            if retrieved_docs:
                context = "\n\n".join(
                    f"[ì¶œì²˜: {doc['source']}] {doc['content'][:500]}" for doc in retrieved_docs[:3]
                )
                print(f"âœ… íŒ©íŠ¸ ì²´í¬ìš© ë¬¸ì„œ {len(retrieved_docs)}ê°œ ê²€ìƒ‰ ì™„ë£Œ")
            else:
                print("ğŸ“ ê´€ë ¨ íŒ©íŠ¸ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            print("ğŸ’­ ì¼ë°˜ì ì¸ ëŒ€í™” ëª¨ë“œ (íŒ©íŠ¸ ì²´í¬ ë¶ˆí•„ìš”)")

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ìƒì„± (ìµœê·¼ 5ê°œê¹Œì§€ë§Œ ì‚¬ìš©)
        history = "\n".join(state["dialog_log"][-5:])

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
ë‹¹ì‹ ì€ SK ì„±ê³¼ê´€ë¦¬ AI ì±—ë´‡ìœ¼ë¡œì„œ, ì‚¬ìš©ìê°€ í‰ê°€ì— ëŒ€í•´ ëŠë‚€ ì–µìš¸í•¨ì´ë‚˜ ë¬¸ì œì˜ì‹ì„ ëª…í™•íˆ í‘œí˜„í•˜ë„ë¡ ë•ëŠ” ì—­í• ì…ë‹ˆë‹¤.
ì§ì ‘ì ì¸ ì •ë‹µ ì œê³µê³¼ ë™ì‹œì— ê³µê°ê³¼ ì§ˆë¬¸ì„ í†µí•´ ì‚¬ìš©ìì˜ ì…ì¥ì„ ë” ëŒì–´ë‚´ì„¸ìš”.

[ëŒ€í™” ì´ë ¥]
{history}

[ì‚¬ìš©ì ìµœì‹  ì§ˆë¬¸]
{query}

[ì°¸ê³  ë¬¸ì„œ]
{context}

[ì‘ë‹µ ê°€ì´ë“œ]
- ê³µê° ë˜ëŠ” ì§ˆë¬¸ìœ¼ë¡œ ì‹œì‘
- ì‚¬ìš©ìê°€ ë” ìì„¸íˆ ì„¤ëª…í•˜ê²Œ ìœ ë„
- ë¶€ë“œëŸ½ê³  ì¹œì ˆí•œ ë§íˆ¬ ì‚¬ìš©
- ë¬¸ì„œ ê¸°ì¤€ ë‚´ìš©ì´ ìˆìœ¼ë©´ ë¶€ë“œëŸ½ê²Œ ë°˜ì˜ ("ë¬¸ì„œ ê¸°ì¤€ìœ¼ë¡œ ë³´ë©´ ~ì¼ ìˆ˜ ìˆì–´ìš”.")
- user_id(emp_no)ì— í•´ë‹¹í•˜ëŠ” ì§ì›, ì¦‰ ë³¸ì¸ ì™¸ì— ë‹¤ë¥¸ ì§ì›ì€ ìµëª…ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""

        # LLM í˜¸ì¶œ
        if llm:
            print("ğŸ¤– LLM ì‘ë‹µ ìƒì„± ì¤‘...")
            try:
                llm_response = llm.predict(prompt)
                print(f"âœ… LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ ({len(llm_response)}ì)")
            except Exception as e:
                print(f"âš ï¸ LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
                llm_response = f"ì£„ì†¡í•©ë‹ˆë‹¤. '{query}'ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        else:
            llm_response = f"í˜„ì¬ ì‹œìŠ¤í…œ ì ê²€ ì¤‘ì…ë‹ˆë‹¤. '{query}'ì— ëŒ€í•œ ë‹µë³€ì€ ì ì‹œ í›„ ì œê³µí•˜ê² ìŠµë‹ˆë‹¤."

        # ìƒíƒœ ì—…ë°ì´íŠ¸
        state["dialog_log"].append(f"ì±—ë´‡: {llm_response.strip()}")
        state["llm_response"] = llm_response.strip()
        state["retrieved_docs"] = retrieved_docs

        print("âœ… ì´ì˜ì œê¸° ëŒ€í™” ë…¸ë“œ ì™„ë£Œ")

    except ImportError as e:
        print(f"âš ï¸ RAG ëª¨ë“ˆ import ì‹¤íŒ¨: {str(e)}")
        # ê¸°ë³¸ ì‘ë‹µ fallback
        simple_response = "í˜„ì¬ ì¼ë¶€ ê¸°ëŠ¥ì´ ì ê²€ ì¤‘ì…ë‹ˆë‹¤. ê¸°ë³¸ì ì¸ ìƒë‹´ì€ ê°€ëŠ¥í•˜ë‹ˆ ë§ì”€í•´ì£¼ì„¸ìš”."
        state["retrieved_docs"] = []
        state["llm_response"] = simple_response
        state["dialog_log"].append(f"ì±—ë´‡: {simple_response}")
        
    except Exception as e:
        print(f"âŒ ì´ì˜ì œê¸° ëŒ€í™” ë…¸ë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        error_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        state["retrieved_docs"] = []
        state["llm_response"] = error_response
        state["dialog_log"].append(f"ì±—ë´‡: {error_response}")

    return state


def summary_generator_node(state: ChatState) -> ChatState:
    """ìš”ì•½ ìƒì„± ë…¸ë“œ (ì›ë³¸ ë¡œì§ ê·¸ëŒ€ë¡œ)"""
    conversation = "\n".join(state["dialog_log"])

    prompt = f"""
ë‹¹ì‹ ì€ SKê·¸ë£¹ ì„±ê³¼í‰ê°€ ì‹œìŠ¤í…œì˜ AI ìš”ì•½ ì±—ë´‡ì…ë‹ˆë‹¤.
ì•„ë˜ ëŒ€í™”ëŠ” í•œ êµ¬ì„±ì›ê³¼ ì±—ë´‡ ê°„ì˜ ì´ì˜ì œê¸° ëŒ€í™”ì…ë‹ˆë‹¤.

ì´ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ, **ì•„ì§ í•´ê²°ë˜ì§€ ì•Šì€ ì˜ë¬¸**ì´ë‚˜ **ì´ì˜ì œê¸° ë‚´ìš©ì„ ìš”ì•½**í•´ ì£¼ì„¸ìš”.
ë‹¨, ë‹¤ìŒ ì¡°ê±´ì„ ë°˜ë“œì‹œ ì§€ì¼œì£¼ì„¸ìš”:

[ì‘ì„± ì¡°ê±´]
- ëˆ„êµ¬ì¸ì§€ ì‹ë³„í•  ìˆ˜ ì—†ë„ë¡ **ì™„ì „í•œ ìµëª… í‘œí˜„** ì‚¬ìš© (1ì¸ì¹­, ì‹¤ëª…, ì§ë¬´ëª…, íŒ€ëª… ê¸ˆì§€)
- ìš•ì„¤, ë¶ˆë§Œì€ ì •ì¤‘í•˜ê²Œ ì •ì œ
- **ë‘ê´„ì‹**ìœ¼ë¡œ ìš”ì•½ (í•µì‹¬ ì£¼ì¥ â†’ ìƒì„¸ ì„¤ëª…)
- ì—¬ì „íˆ ì˜ë¬¸ì´ ë‚¨ëŠ” ë‚´ìš©ë§Œ ìš”ì•½
- **ìˆ˜ê¸í•œ ë‚´ìš©ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”**
- ë¬¸ì„œ ê¸°ë°˜ ì‚¬ì‹¤ê´€ê³„ì— ëŒ€í•œ ê·¼ê±°ëŠ” ìš”ì•½í•˜ì§€ ì•ŠìŒ (ëŒ€í™” ì¤‘ ìˆ˜ê¸í•œ ë‚´ìš©ì€ ì œì™¸)
- **ì˜ë¬¸ì´ êµ¬ì²´ì ìœ¼ë¡œ ë“œëŸ¬ë‚˜ë„ë¡** ì„œìˆ 
- ìš”ì•½ë¬¸ì€ ì˜ˆì˜ì™€ ë…¼ë¦¬ë¥¼ ê°–ì¶”ì–´ì•¼ í•©ë‹ˆë‹¤.

[ëŒ€í™” ê¸°ë¡]
{conversation}

[ìµœì¢… ì¶œë ¥]
- íŒ€ì¥ì´ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±ì›ì´ ì œê¸°í•œ í•µì‹¬ ì˜ë¬¸ì„ **ì •ì¤‘í•˜ê³  ë…¼ë¦¬ì ìœ¼ë¡œ ìš”ì•½**í•œ ë¬¸ì¥ë§Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
- **êµ¬ì„±ì›ì˜ ë§íˆ¬ë‚˜ ê°œì„±ì„ í‰ë‚´ë‚´ì§€ ë§ê³ **, ê°ê´€ì ì¸ í–‰ì •ìš© ë³´ê³  ìŠ¤íƒ€ì¼ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
"""

    # LLM í˜¸ì¶œ
    llm_response = config.llm.predict(prompt)

    state["summary_draft"] = llm_response.strip()
    state["llm_response"] = llm_response.strip()

    return state

# def summary_generator_node(state: ChatState) -> ChatState:
#     """ìš”ì•½ ìƒì„±"""
#     print("ğŸ“ ìš”ì•½ ìƒì„± ì¤‘...")
    
#     conversation = "\n".join(state["dialog_log"])
    
#     # ê°„ë‹¨í•œ ìš”ì•½
#     simple_summary = f"ì´ì˜ì œê¸° ìš”ì•½: ì‚¬ìš©ìê°€ ì œê¸°í•œ ì£¼ìš” ì˜ë¬¸ì‚¬í•­ë“¤ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤."
    
#     # LLM ì‚¬ìš© (ê°€ëŠ¥í•œ ê²½ìš°)
#     if llm:
#         try:
#             prompt = f"""
# ë‹¤ìŒ ì´ì˜ì œê¸° ëŒ€í™”ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”:

# {conversation}

# ê°ê´€ì ì´ê³  ì •ì¤‘í•œ í†¤ìœ¼ë¡œ í•µì‹¬ ì˜ë¬¸ì‚¬í•­ë§Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
# """
#             llm_response = llm.predict(prompt)
#         except Exception as e:
#             print(f"âš ï¸ LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
#             llm_response = simple_summary
#     else:
#         llm_response = simple_summary
    
#     state["summary_draft"] = llm_response.strip()
#     state["llm_response"] = llm_response.strip()
    
#     return state

# =============================================================================
# LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±
# =============================================================================

def create_chatbot_workflow():
    """ì±—ë´‡ ì›Œí¬í”Œë¡œìš° ìƒì„±"""
    print("ğŸ”„ LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„± ì¤‘...")
    
    workflow = StateGraph(ChatState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("initialize", initialize_state)
    workflow.add_node("qna_agent", qna_agent_node)
    workflow.add_node("appeal_dialogue", appeal_dialogue_node)
    workflow.add_node("summary_generator", summary_generator_node)
    
    # ì‹œì‘ì  ì„¤ì •
    workflow.add_edge(START, "initialize")
    
    # ì¡°ê±´ë¶€ ì—£ì§€ (ë¼ìš°íŒ…)
    workflow.add_conditional_edges(
        "initialize",
        route_chat_mode,
        {
            "qna_agent": "qna_agent",
            "appeal_dialogue": "appeal_dialogue",
            "summary_generator": "summary_generator"
        }
    )
    
    # ëª¨ë“  ë…¸ë“œëŠ” ENDë¡œ ì¢…ë£Œ
    workflow.add_edge("qna_agent", END)
    workflow.add_edge("appeal_dialogue", END)
    workflow.add_edge("summary_generator", END)
    
    compiled_workflow = workflow.compile()
    print("âœ… LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„± ì™„ë£Œ!")
    
    return compiled_workflow