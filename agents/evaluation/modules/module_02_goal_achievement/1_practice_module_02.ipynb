{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b383f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f92fa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Client initialized with model: gpt-4o-mini, temperature: 0.0\n"
     ]
    }
   ],
   "source": [
    "# ai-performance-management-system/모듈2_구현_테스트.ipynb\n",
    "\n",
    "# 1. 환경 설정 및 라이브러리 설치 (필요시)\n",
    "# 이미 설치했다면 이 셀은 건너뛰세요.\n",
    "# !pip install langchain langchain-openai langgraph sqlalchemy pymysql python-dotenv\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../../../..')))  # 루트 경로로 이동\n",
    "\n",
    "from config.settings import DatabaseConfig\n",
    "from agents.evaluation.modules.module_02_goal_achievement import db_utils\n",
    "\n",
    "# 2. .env 파일 로드 (DB 접속 정보 설정 시 필요)\n",
    "# 실제 DB 연결을 위해 DB_USERNAME, DB_PASSWORD 등을 .env 파일에 설정해야 합니다.\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# 3. LangSmith 추적 설정 (선택 사항 - 디버깅에 매우 유용)\n",
    "# from langchain_teddynote import logging\n",
    "# logging.langsmith(\"CH21-Module2-Local-Test\")\n",
    "\n",
    "# 4. 필수 라이브러리 임포트\n",
    "from typing import Annotated, List, Literal, TypedDict, Dict, Any, Optional\n",
    "from langchain_core.messages import HumanMessage # LangChain 메시지 타입\n",
    "import operator # operator.add 사용\n",
    "from langgraph.graph import StateGraph, START, END # LangGraph 그래프 구성\n",
    "import uuid # 고유 ID 생성을 위해 (thread_id 등)\n",
    "import random # Mocking 함수에서 랜덤 값 생성을 위해\n",
    "\n",
    "# SQLAlchemy 관련 임포트 (DB 연동 시 필요)\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.engine import Connection, Row\n",
    "\n",
    "# config.settings (DB 접속 정보는 여기서 가져옵니다)\n",
    "# 실제 파일 경로에 따라 임포트 방식이 달라질 수 있습니다.\n",
    "# 만약 주피터 노트북 파일이 프로젝트 루트에 있다면:\n",
    "from config.settings import DatabaseConfig \n",
    "from agents.evaluation.modules.module_02_goal_achievement.db_utils import *\n",
    "# 만약 다른 경로에 있다면 sys.path.append() 후 임포트해야 합니다.\n",
    "# import sys\n",
    "# import os\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # 예시: 현재 디렉토리의 부모를 추가\n",
    "# from config.settings import DatabaseConfig\n",
    "\n",
    "\n",
    "# 5. DB 설정 및 엔진 생성 (실제 DB 연결)\n",
    "db_config = DatabaseConfig()\n",
    "DATABASE_URL = db_config.DATABASE_URL\n",
    "engine = create_engine(DATABASE_URL, pool_pre_ping=True)\n",
    "\n",
    "# --- Module2AgentState 정의 ---\n",
    "class Module2AgentState(TypedDict):\n",
    "   \"\"\"\n",
    "   모듈 2 (목표달성도 분석 모듈)의 내부 상태를 정의합니다.\n",
    "   이 상태는 모듈 2 내의 모든 서브모듈이 공유하고 업데이트합니다.\n",
    "   \"\"\"\n",
    "   messages: Annotated[List[HumanMessage], operator.add] \n",
    "\n",
    "   report_type: Literal[\"quarterly\", \"annual\"] \n",
    "   team_id: int \n",
    "   period_id: int \n",
    "   \n",
    "   target_task_ids: List[int] \n",
    "   target_team_kpi_ids: List[int] \n",
    "\n",
    "   updated_task_ids: List[int]\n",
    "   updated_team_kpi_ids: List[int]\n",
    "   \n",
    "   kpi_individual_relative_contributions: List[Dict] = [] \n",
    "\n",
    "   feedback_report_id: int = None \n",
    "   team_evaluation_id: int = None \n",
    "   final_evaluation_report_id: int = None \n",
    "   updated_temp_evaluation_ids_list: List[int] = [] \n",
    "\n",
    "\n",
    "\n",
    "# ai-performance-management-system/shared/tools/py\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.engine import Connection, Row\n",
    "from typing import Optional, List, Dict, Any\n",
    "from config.settings import DatabaseConfig\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import random \n",
    "import json \n",
    "import re \n",
    "\n",
    "# LangChain LLM 관련 임포트\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "\n",
    "load_dotenv() \n",
    "\n",
    "# --- LLM 클라이언트 인스턴스 (전역 설정) ---\n",
    "llm_client = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0) \n",
    "print(f\"LLM Client initialized with model: {llm_client.model_name}, temperature: {llm_client.temperature}\")\n",
    "\n",
    "# 데이터베이스 설정 로드\n",
    "db_config = DatabaseConfig()\n",
    "DATABASE_URL = db_config.DATABASE_URL\n",
    "\n",
    "# SQLAlchemy Engine 생성\n",
    "engine = create_engine(DATABASE_URL, pool_pre_ping=True)\n",
    "\n",
    "# --- 도우미 함수: SQLAlchemy Row 객체를 딕셔너리로 변환 ---\n",
    "def row_to_dict(row: Row) -> Dict[str, Any]:\n",
    "   \"\"\"SQLAlchemy Row 객체를 딕셔너리로 변환합니다.\"\"\"\n",
    "   if row is None:\n",
    "       return {}\n",
    "   return row._asdict() # ._asdict() 사용\n",
    "\n",
    "\n",
    "# --- LLM 응답에서 JSON 코드 블록 추출 도우미 함수 ---\n",
    "def _extract_json_from_llm_response(text: str) -> str:\n",
    "   \"\"\"LLM 응답 텍스트에서 ```json ... ``` 블록만 추출합니다.\"\"\"\n",
    "   match = re.search(r\"```(?:json)?\\s*(.*?)\\s*```\", text, re.DOTALL)\n",
    "   if match:\n",
    "       return match.group(1).strip() # JSON 내용만 반환하고 양쪽 공백 제거\n",
    "   return text.strip()\n",
    "\n",
    "\n",
    "# --- 데이터 조회 함수 (`SELECT` 쿼리 구현) ---\n",
    "def fetch_task_summary_by_id(task_summary_id: int) -> Optional[Dict]:\n",
    "   \"\"\"\n",
    "   `task_summary_Id`로 `task_summaries` 및 관련 `tasks`, `employees` 테이블에서 상세 Task Summary 데이터를 조회합니다.\n",
    "   \"\"\"\n",
    "   with engine.connect() as connection:\n",
    "       query = text(f\"\"\"\n",
    "           SELECT ts.*, t.task_name, t.target_level, t.emp_no, t.team_kpi_id, t.weight, e.emp_name,\n",
    "                  t.ai_contribution_score, t.ai_achievement_rate, t.ai_assessed_grade, t.ai_analysis_comment_task\n",
    "           FROM task_summaries ts\n",
    "           JOIN tasks t ON ts.task_id = t.task_id\n",
    "           JOIN employees e ON t.emp_no = e.emp_no \n",
    "           WHERE ts.task_summary_Id = :task_summary_id\n",
    "       \"\"\")\n",
    "       result = connection.execute(query, {\"task_summary_id\": task_summary_id}).fetchone()\n",
    "       return row_to_dict(result) if result else None\n",
    "\n",
    "def fetch_task_summaries_by_task_and_period(task_id: int, period_id: int) -> List[Dict]:\n",
    "    \"\"\"특정 Task의 해당 분기까지 모든 Task Summary 조회\"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        query = text(\"\"\"\n",
    "            SELECT ts.*, t.task_name, t.target_level, t.weight, t.emp_no, t.team_kpi_id, e.emp_name\n",
    "            FROM task_summaries ts\n",
    "            JOIN tasks t ON ts.task_id = t.task_id\n",
    "            JOIN employees e ON t.emp_no = e.emp_no \n",
    "            WHERE ts.task_id = :task_id AND ts.period_id <= :period_id\n",
    "            ORDER BY ts.period_id\n",
    "        \"\"\")\n",
    "        results = connection.execute(query, {\"task_id\": task_id, \"period_id\": period_id}).fetchall()\n",
    "        return [row_to_dict(row) for row in results]\n",
    "\n",
    "def fetch_kpi_data_by_id(team_kpi_id: int) -> Optional[Dict]:\n",
    "   \"\"\"\n",
    "   `team_kpi_id`로 `team_kpis` 테이블에서 상세 KPI 데이터를 조회합니다.\n",
    "   \"\"\"\n",
    "   with engine.connect() as connection:\n",
    "       query = text(\"SELECT * FROM team_kpis WHERE team_kpi_id = :team_kpi_id\")\n",
    "       result = connection.execute(query, {\"team_kpi_id\": team_kpi_id}).fetchone()\n",
    "       return row_to_dict(result) if result else None\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "def fetch_tasks_for_kpi(team_kpi_id: int, period_id: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    특정 KPI에 속한 Task들을 조회합니다.\n",
    "    \"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        query = text(\"\"\"\n",
    "            SELECT t.task_id, t.task_name, t.target_level, t.weight, t.emp_no, \n",
    "                   ts.task_summary, ts.task_performance, ts.task_summary_Id, \n",
    "                   ts.period_id,  -- 🔧 추가: period_id를 SELECT에 포함\n",
    "                   e.emp_name, ts.ai_contribution_score, ts.ai_achievement_rate,\n",
    "                   ts.ai_assessed_grade, ts.ai_analysis_comment_task\n",
    "            FROM tasks t\n",
    "            JOIN task_summaries ts ON t.task_id = ts.task_id\n",
    "            JOIN employees e ON t.emp_no = e.emp_no\n",
    "            WHERE t.team_kpi_id = :team_kpi_id AND ts.period_id <= :period_id\n",
    "        \"\"\")\n",
    "        \n",
    "        results = connection.execute(query, {\"team_kpi_id\": team_kpi_id, \"period_id\": period_id}).fetchall()\n",
    "        return [row_to_dict(row) for row in results]\n",
    "\n",
    "def fetch_cumulative_task_data_for_comment(task_id: int, period_id: int) -> Dict:\n",
    "    \"\"\"\n",
    "    task_id의 모든 분기 정보를 누적해서 반환 (코멘트 생성용)\n",
    "    \"\"\"\n",
    "    # 모든 분기 데이터 조회\n",
    "    task_summaries = fetch_task_summaries_by_task_and_period(task_id, period_id)\n",
    "    if not task_summaries:\n",
    "        return {}\n",
    "    \n",
    "    # 최신 기본 정보 + 누적 요약\n",
    "    latest_data = task_summaries[-1]\n",
    "    combined_summary = \"\\n\".join([f\"Q{ts['period_id']}: {ts['task_summary']}\" for ts in task_summaries])\n",
    "    \n",
    "    return {\n",
    "        **latest_data,  # 기본 정보는 최신 것 사용\n",
    "        \"cumulative_task_summary\": combined_summary,  # 누적 요약 추가\n",
    "        \"cumulative_task_performance\": latest_data.get('task_performance', '')\n",
    "    }\n",
    "\n",
    "def fetch_grade_definitions_from_db() -> Dict:\n",
    "   \"\"\"\n",
    "   `grades` 테이블에서 LLM이 참고할 등급 정의 (`grade_s`, `grade_a` 등 컬럼의 텍스트)를 조회합니다.\n",
    "   \"\"\"\n",
    "   with engine.connect() as connection:\n",
    "       query = text(\"SELECT grade_id, grade_s, grade_a, grade_b, grade_c, grade_d, grade_rule FROM grades\")\n",
    "       results = connection.execute(query).fetchall()\n",
    "       \n",
    "       if results:\n",
    "           first_row = row_to_dict(results[0])\n",
    "           return {\n",
    "               \"S\": first_row.get(\"grade_s\", \"목표를 초과 달성\"),\n",
    "               \"A\": first_row.get(\"grade_a\", \"목표를 완벽하게 달성하며 높은 품질의 결과물 제공\"),\n",
    "               \"B\": first_row.get(\"grade_b\", \"목표 수준을 정확히 달성\"),\n",
    "               \"C\": first_row.get(\"grade_c\", \"목표에 미달했으나 일부 성과 달성\"),\n",
    "               \"D\": first_row.get(\"grade_d\", \"목표 달성 미흡\")\n",
    "           }\n",
    "       return {}\n",
    "\n",
    "\n",
    "def fetch_team_evaluation_id_by_team_and_period(team_id: int, period_id: int) -> Optional[int]:\n",
    "   \"\"\"\n",
    "   `team_evaluations` 테이블에서 `team_id`와 `period_id`로 `team_evaluation_id`를 조회합니다.\n",
    "   Spring에서 이 레코드를 미리 생성하고 ID를 관리한다고 가정합니다.\n",
    "   \"\"\"\n",
    "   with engine.connect() as connection:\n",
    "       query = text(\"SELECT team_evaluation_id FROM team_evaluations WHERE team_id = :team_id AND period_id = :period_id\")\n",
    "       result = connection.execute(query, {\"team_id\": team_id, \"period_id\": period_id}).scalar_one_or_none()\n",
    "       return result\n",
    "\n",
    "\n",
    "\n",
    "def fetch_employees_by_team_id(team_id: int) -> List[Dict]:\n",
    "   \"\"\"\n",
    "   특정 팀에 속한 모든 직원의 emp_no, emp_name, role을 조회합니다.\n",
    "   \"\"\"\n",
    "   with engine.connect() as connection:\n",
    "       query = text(\"SELECT emp_no, emp_name, role FROM employees WHERE team_id = :team_id\")\n",
    "       results = connection.execute(query, {\"team_id\": team_id}).fetchall()\n",
    "       return [row_to_dict(row) for row in results]\n",
    "\n",
    "# --- 데이터 업데이트/추가 함수 (`UPDATE` / `INSERT` 쿼리 구현) ---\n",
    "\n",
    "def update_task_summary_ai_results_in_db(task_summary_id: int, update_data: Dict) -> bool:\n",
    "    \"\"\"\n",
    "    `task_summary_Id`에 해당하는 `task_summaries` 테이블 레코드의 AI 컬럼들을 업데이트합니다.\n",
    "    \"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        set_clauses = [f\"`{k}` = :{k}\" for k in update_data.keys()]\n",
    "        query = text(f\"UPDATE `task_summaries` SET {', '.join(set_clauses)} WHERE `task_summary_Id` = :task_summary_id\")\n",
    "        \n",
    "        params = {**update_data, \"task_summary_id\": task_summary_id}\n",
    "        result = connection.execute(query, params)\n",
    "        connection.commit()\n",
    "        return result.rowcount > 0\n",
    "\n",
    "\n",
    "\n",
    "def update_team_kpi_ai_results_in_db(team_kpi_id: int, update_data: Dict) -> bool:\n",
    "    \"\"\"\n",
    "    `team_kpi_id`에 해당하는 `team_kpis` 테이블 레코드의 AI 컬럼들을 업데이트합니다.\n",
    "    \"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        set_clauses = [f\"`{k}` = :{k}\" for k in update_data.keys()]\n",
    "        query = text(f\"UPDATE `team_kpis` SET {', '.join(set_clauses)} WHERE `team_kpi_id` = :team_kpi_id\")\n",
    "        \n",
    "        params = {**update_data, \"team_kpi_id\": team_kpi_id}\n",
    "        result = connection.execute(query, params)\n",
    "        connection.commit()\n",
    "        return result.rowcount > 0\n",
    "\n",
    "def save_feedback_report_module2_results_to_db(emp_no: str, team_evaluation_id: int, results: Dict) -> int: \n",
    "    \"\"\"\n",
    "    `feedback_reports` 테이블에 모듈 2 관련 AI 결과를 삽입하거나 업데이트합니다.\n",
    "    \"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        cols_for_insert = [\"emp_no\", \"team_evaluation_id\"] + list(results.keys())\n",
    "        values_placeholder = \", \".join([f\":{col}\" for col in cols_for_insert])\n",
    "        cols_str = \", \".join([f\"`{col}`\" for col in cols_for_insert])\n",
    "\n",
    "        on_duplicate_set_clauses = [f\"`{k}` = VALUES(`{k}`)\" for k in results.keys()]\n",
    "        \n",
    "        query = text(f\"\"\"\n",
    "            INSERT INTO `feedback_reports` ({cols_str}) VALUES ({values_placeholder})\n",
    "            ON DUPLICATE KEY UPDATE {\", \".join(on_duplicate_set_clauses)}\n",
    "        \"\"\")\n",
    "        \n",
    "        params = {\"emp_no\": emp_no, \"team_evaluation_id\": team_evaluation_id, **results} \n",
    "        \n",
    "        connection.execute(query, params)\n",
    "        connection.commit()\n",
    "        \n",
    "        inserted_or_updated_id_query = text(\"\"\"\n",
    "            SELECT feedback_report_id FROM `feedback_reports`\n",
    "            WHERE `emp_no` = :emp_no AND `team_evaluation_id` = :team_evaluation_id\n",
    "        \"\"\")\n",
    "        ret_id = connection.execute(inserted_or_updated_id_query, {\"emp_no\": emp_no, \"team_evaluation_id\": team_evaluation_id}).scalar_one()\n",
    "        \n",
    "        print(f\"DB: feedback_reports[{ret_id}] for emp_no={emp_no}, team_evaluation_id={team_evaluation_id} inserted/updated.\")\n",
    "        return ret_id\n",
    "    \n",
    "\n",
    "def update_team_evaluations_module2_results_in_db(team_evaluation_id: int, update_data: Dict) -> bool:\n",
    "    \"\"\"\n",
    "    `team_evaluations` 테이블에 모듈 2 관련 AI 결과를 업데이트합니다.\n",
    "    \"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        set_clauses = [f\"`{k}` = :{k}\" for k in update_data.keys()]\n",
    "        query = text(f\"UPDATE `team_evaluations` SET {', '.join(set_clauses)} WHERE `team_evaluation_id` = :team_evaluation_id\")\n",
    "        \n",
    "        params = {**update_data, \"team_evaluation_id\": team_evaluation_id}\n",
    "        result = connection.execute(query, params)\n",
    "        connection.commit()\n",
    "        return result.rowcount > 0\n",
    "\n",
    "def save_final_evaluation_report_module2_results_to_db(emp_no: str, team_evaluation_id: int, results: Dict) -> int:\n",
    "    \"\"\"\n",
    "    `final_evaluation_reports` 테이블에 모듈 2 관련 AI 결과를 삽입하거나 업데이트합니다.\n",
    "    \"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        cols_for_insert = [\"emp_no\", \"team_evaluation_id\"] + list(results.keys())\n",
    "        values_placeholder = \", \".join([f\":{col}\" for col in cols_for_insert])\n",
    "        cols_str = \", \".join([f\"`{col}`\" for col in cols_for_insert])\n",
    "        \n",
    "        on_duplicate_set_clauses = [f\"`{k}` = VALUES(`{k}`)\" for k in results.keys()]\n",
    "        \n",
    "        query = text(f\"\"\"\n",
    "            INSERT INTO `final_evaluation_reports` ({cols_str}) VALUES ({values_placeholder})\n",
    "            ON DUPLICATE KEY UPDATE {\", \".join(on_duplicate_set_clauses)}\n",
    "        \"\"\")\n",
    "        \n",
    "        params = {\"emp_no\": emp_no, \"team_evaluation_id\": team_evaluation_id, **results}\n",
    "        \n",
    "        connection.execute(query, params)\n",
    "        connection.commit()\n",
    "        \n",
    "        inserted_or_updated_id_query = text(\"\"\"\n",
    "            SELECT final_evaluation_report_id FROM `final_evaluation_reports`\n",
    "            WHERE `emp_no` = :emp_no AND `team_evaluation_id` = :team_evaluation_id\n",
    "        \"\"\")\n",
    "        ret_id = connection.execute(inserted_or_updated_id_query, {\"emp_no\": emp_no, \"team_evaluation_id\": team_evaluation_id}).scalar_one()\n",
    "        \n",
    "        print(f\"DB: final_evaluation_reports[{ret_id}] created/updated for emp_no={emp_no}.\")\n",
    "        return ret_id\n",
    "\n",
    "\n",
    "\n",
    "# --- LLM 호출 함수들 ---\n",
    "\n",
    "\n",
    "\n",
    "def call_llm_for_task_achievement(target_level_text: str, task_performance_text: str, grade_definitions: Dict) -> Dict:\n",
    "   \n",
    "   system_prompt = \"\"\"\n",
    "   당신은 SK 조직의 성과 평가 전문가입니다.\n",
    "   아래 Task 목표와 실제 성과를 비교하여, Task의 달성률(0-100점 이상)과 적절한 등급(S, A, B, C, D)을 판단하고,\n",
    "   상세 분석 코멘트를 생성해주세요.\n",
    "\n",
    "   평가 기준은 다음과 같습니다:\n",
    "   - 달성률은 0점부터 시작하며, 100점을 초과하여 목표 초과 달성을 나타낼 수 있습니다. (예: 100.1% 이상)\n",
    "   - 등급은 S, A, B, C, D 중 하나여야 합니다.\n",
    "\n",
    "   <등급 정의 (LLM 참고용)>\n",
    "   \"\"\"\n",
    "   for grade, desc in grade_definitions.items():\n",
    "       system_prompt += f\"- {grade} 등급: {desc}\\n\"\n",
    "   system_prompt += \"</등급 정의>\\n\"\n",
    "   system_prompt += \"결과는 다음 JSON 형식으로만 응답해주세요. 불필요한 서문이나 추가 설명 없이 JSON만 반환해야 합니다.\"\n",
    "\n",
    "   human_prompt = f\"\"\"\n",
    "   <Task 목표>\n",
    "   {target_level_text}\n",
    "   </Task 목표>\n",
    "\n",
    "   <실제 성과>\n",
    "   {task_performance_text}\n",
    "   </실제 성과>\n",
    "\n",
    "   JSON 응답:\n",
    "   {{\n",
    "     \"달성률\": [달성률 (0-100점 이상)],\n",
    "     \"등급\": \"[S, A, B, C, D 중 하나]\",\n",
    "     \"상세 분석 코멘트\": \"[Task에 대한 상세 분석 코멘트]\"\n",
    "   }}\n",
    "   \"\"\"\n",
    "   \n",
    "   prompt = ChatPromptTemplate.from_messages([\n",
    "       SystemMessage(content=system_prompt),\n",
    "       HumanMessage(content=human_prompt)\n",
    "   ])\n",
    "   \n",
    "   chain = prompt | llm_client\n",
    "\n",
    "   try:\n",
    "       response: AIMessage = chain.invoke({\"target_level_text\": target_level_text, \"task_performance_text\": task_performance_text, \"grade_definitions\": grade_definitions})\n",
    "       json_output_raw = response.content\n",
    "       \n",
    "       json_output = _extract_json_from_llm_response(json_output_raw)\n",
    "       \n",
    "       llm_parsed_data = json.loads(json_output)\n",
    "       \n",
    "       rate = llm_parsed_data.get(\"달성률\") \n",
    "       grade = llm_parsed_data.get(\"등급\") \n",
    "       analysis = llm_parsed_data.get(\"상세 분석 코멘트\") \n",
    "\n",
    "       # 수정된 부분: 달성률 유효성 검사 상한 제거\n",
    "       if not isinstance(rate, (int, float)) or not (0 <= rate): \n",
    "           raise ValueError(f\"LLM 반환 달성률 {rate}가 유효하지 않습니다 (0 이상이어야 합니다).\")\n",
    "       if grade not in [\"S\", \"A\", \"B\", \"C\", \"D\"]:\n",
    "           raise ValueError(f\"LLM 반환 등급 {grade}가 유효하지 않습니다.\")\n",
    "       if not isinstance(analysis, str) or not analysis:\n",
    "           raise ValueError(f\"LLM 반환 분석 코멘트 {analysis}가 유효하지 않습니다.\")\n",
    "\n",
    "       return {\"grade\": grade, \"rate\": round(float(rate), 2), \"analysis\": analysis}\n",
    "\n",
    "   except json.JSONDecodeError as e:\n",
    "       print(f\"LLM 응답 JSON 파싱 오류: {e}. 원본 응답: '{json_output_raw}'. 파싱 시도 텍스트: '{json_output[:100]}...'\")\n",
    "       return {\"grade\": \"D\", \"rate\": 0.0, \"analysis\": f\"AI 분석 실패: JSON 파싱 오류 - {json_output_raw[:100]}...\"}\n",
    "   except ValueError as e:\n",
    "       print(f\"LLM 응답 데이터 유효성 오류: {e}. 원본 응답: '{json_output_raw}'. 파싱 시도 텍스트: '{json_output[:100]}...'\")\n",
    "       return {\"grade\": \"D\", \"rate\": 0.0, \"analysis\": f\"AI 분석 실패: 유효성 오류 - {json_output_raw[:100]}...\"}\n",
    "   except Exception as e:\n",
    "       print(f\"LLM 호출 중 예기치 않은 오류 발생: {e}. 원본 응답: '{json_output_raw}'\")\n",
    "       return {\"grade\": \"D\", \"rate\": 0.0, \"analysis\": f\"AI 분석 실패: 예기치 않은 오류 - {str(e)[:100]}...\"}\n",
    "\n",
    "\n",
    "def call_llm_for_overall_contribution_summary(all_individual_task_results: List[Dict], emp_name: str, emp_no: str) -> Dict: \n",
    "   print(f\"LLM Call (Overall Contribution Summary): '{emp_name} ({emp_no})' Task {len(all_individual_task_results)}개 기반 요약 요청.\") \n",
    "\n",
    "   task_details_str = \"\"\n",
    "   for task in all_individual_task_results:\n",
    "       task_details_str += f\"- Task: {task.get('task_name')} (ID: {task.get('task_id')})\\n\"\n",
    "       task_details_str += f\"  Summary: {task.get('task_summary', task.get('task_performance', ''))}\\n\"\n",
    "       if task.get('ai_contribution_score') is not None:\n",
    "           task_details_str += f\"  AI 기여도: {task.get('ai_contribution_score')}점\\n\"\n",
    "       if task.get('ai_achievement_rate') is not None:\n",
    "           task_details_str += f\"  AI 달성률: {task.get('ai_achievement_rate')}%\\n\"\n",
    "       if task.get('ai_assessed_grade'):\n",
    "           task_details_str += f\"  AI 등급: {task.get('ai_assessed_grade')}\\n\"\n",
    "       task_details_str += \"\\n\"\n",
    "\n",
    "   system_prompt = \"\"\"\n",
    "   당신은 SK 조직의 HR 성과 전문가입니다.\n",
    "   아래 제공된 개인의 모든 Task 정보, Task Summary, 그리고 AI가 분석한 개별 Task 기여도/달성률 점수를 종합적으로 고려하여,\n",
    "   이 개인의 총체적인 기여도 점수 (팀 내 상대 비율, 0-100%)를 추정하고,\n",
    "   이름과 사번을 명시하며 개인의 전체적인 성과와 기여에 대한 간략한 종합 코멘트를 생성해주세요.\n",
    "\n",
    "   결과는 다음 JSON 형식으로만 응답해주세요. 불필요한 서문이나 추가 설명 없이 JSON만 반환해야 합니다.\n",
    "   직원 이름을 언급할 때는 반드시 \"이름(사번)님\" 형태로 작성해주세요.\n",
    "   \"\"\"\n",
    "\n",
    "   human_prompt = f\"\"\"\n",
    "   <개인 Task 종합 정보>\n",
    "   {task_details_str}\n",
    "   </개인 Task 종합 정보>\n",
    "   <평가 대상 개인 정보>\n",
    "   이름: {emp_name}\n",
    "   사번: {emp_no}\n",
    "   </평가 대상 개인 정보>\n",
    "\n",
    "   JSON 응답:\n",
    "   {{\n",
    "     \"total_contribution\": [개인의 총체적인 기여도 점수 (0-100점)],\n",
    "     \"comment\": \"[{emp_name}({emp_no})님의 전체 성과와 기여에 대한 종합 코멘트]\",\n",
    "     \"average_rate\": [Task 달성률들의 평균 또는 종합적인 달성률 추정 (0-100점 이상)]\n",
    "   }}\n",
    "   \"\"\"\n",
    "\n",
    "\n",
    "   \n",
    "   prompt = ChatPromptTemplate.from_messages([\n",
    "       SystemMessage(content=system_prompt),\n",
    "       HumanMessage(content=human_prompt)\n",
    "   ])\n",
    "   \n",
    "   chain = prompt | llm_client\n",
    "\n",
    "   try:\n",
    "       response: AIMessage = chain.invoke({\"all_individual_task_results\": all_individual_task_results, \"emp_name\": emp_name, \"emp_no\": emp_no})\n",
    "       json_output_raw = response.content\n",
    "       \n",
    "       json_output = _extract_json_from_llm_response(json_output_raw)\n",
    "       \n",
    "       llm_parsed_data = json.loads(json_output)\n",
    "       \n",
    "       total_contribution = llm_parsed_data.get(\"total_contribution\")\n",
    "       comment = llm_parsed_data.get(\"comment\")\n",
    "       average_rate = llm_parsed_data.get(\"average_rate\")\n",
    "\n",
    "       if not isinstance(total_contribution, (int, float)) or not (0 <= total_contribution <= 100):\n",
    "           raise ValueError(f\"LLM 반환 총 기여도 {total_contribution}가 유효하지 않습니다.\")\n",
    "       if not isinstance(comment, str) or not comment:\n",
    "           raise ValueError(f\"LLM 반환 코멘트 {comment}가 유효하지 않습니다.\")\n",
    "       if not isinstance(average_rate, (int, float)) or not (0 <= average_rate): # 0-120점 -> 0점 이상으로 수정\n",
    "           raise ValueError(f\"LLM 반환 평균 달성률 {average_rate}가 유효하지 않습니다 (0 이상이어야 합니다).\")\n",
    "\n",
    "       return {\"total_contribution\": round(float(total_contribution), 2), \"comment\": comment, \"average_rate\": round(float(average_rate), 2)}\n",
    "\n",
    "   except json.JSONDecodeError as e:\n",
    "       print(f\"LLM 응답 JSON 파싱 오류: {e}. 원본 응답: '{json_output_raw}'. 파싱 시도 텍스트: '{json_output[:100]}...'\")\n",
    "       return {\"total_contribution\": 0.0, \"comment\": f\"AI 분석 실패: JSON 파싱 오류 - {json_output_raw[:100]}...\", \"average_rate\": 0.0}\n",
    "   except ValueError as e:\n",
    "       print(f\"LLM 응답 데이터 유효성 오류: {e}. 원본 응답: '{json_output_raw}'. 파싱 시도 텍스트: '{json_output[:100]}...'\")\n",
    "       return {\"total_contribution\": 0.0, \"comment\": f\"AI 분석 실패: 유효성 오류 - {json_output_raw[:100]}...\", \"average_rate\": 0.0}\n",
    "   except Exception as e:\n",
    "       print(f\"LLM 호출 중 예기치 않은 오류 발생: {e}. 원본 응답: '{json_output_raw}'\")\n",
    "       return {\"total_contribution\": 0.0, \"comment\": f\"AI 분석 실패: 예기치 않은 오류 - {str(e)[:100]}...\", \"average_rate\": 0.0}\n",
    "\n",
    "\n",
    "def call_llm_for_cumulative_task_contribution(\n",
    "   task_id: int,\n",
    "   combined_summary: str, \n",
    "   cumulative_performance: str,\n",
    "   target_level: str,\n",
    "   emp_name: str\n",
    ") -> Dict:\n",
    "   \n",
    "   print(f\"LLM Call (Cumulative Task Contribution): Task {task_id} for {emp_name}\")\n",
    "\n",
    "   system_prompt = \"\"\"\n",
    "   당신은 SK 조직의 성과 평가 전문가입니다.\n",
    "   아래는 특정 Task에 대한 분기별 업무 진행 과정과 누적 성과입니다.\n",
    "   \n",
    "   분기별 업무 진행 과정과 누적 성과를 종합적으로 고려하여 해당 Task의 전체적인 기여도를 평가해주세요.\n",
    "   \n",
    "   평가 시 다음을 고려합니다:\n",
    "   - 분기별 업무 진행의 연속성과 발전성\n",
    "   - 누적 성과의 품질과 완성도\n",
    "   - 목표 대비 현재까지의 달성 수준\n",
    "   - Task의 복잡성과 중요도\n",
    "   - 다른 업무나 팀원에게 미친 긍정적 영향\n",
    "   \n",
    "   결과는 다음 JSON 형식으로만 응답해주세요. 불필요한 서문이나 추가 설명 없이 JSON만 반환해야 합니다.\n",
    "   \"\"\"\n",
    "   \n",
    "   human_prompt = f\"\"\"\n",
    "   <Task 기본 정보>\n",
    "   Task ID: {task_id}\n",
    "   담당자: {emp_name}님\n",
    "   목표: {target_level}\n",
    "   </Task 기본 정보>\n",
    "   \n",
    "   <분기별 업무 진행 과정>\n",
    "   {combined_summary}\n",
    "   </분기별 업무 진행 과정>\n",
    "   \n",
    "   <누적 성과>\n",
    "   {cumulative_performance}\n",
    "   </누적 성과>\n",
    "   \n",
    "   JSON 응답:\n",
    "   {{\n",
    "     \"score\": [누적 기여도 점수 (0-100점, 소수점 첫째 자리까지)],\n",
    "     \"comment\": \"[분기별 진행과정과 누적 성과를 종합한 상세 분석 코멘트]\"\n",
    "   }}\n",
    "   \"\"\"\n",
    "   \n",
    "   prompt = ChatPromptTemplate.from_messages([\n",
    "       SystemMessage(content=system_prompt),\n",
    "       HumanMessage(content=human_prompt)\n",
    "   ])\n",
    "   \n",
    "   chain = prompt | llm_client\n",
    "\n",
    "   try:\n",
    "       response: AIMessage = chain.invoke({\n",
    "           \"task_id\": task_id,\n",
    "           \"combined_summary\": combined_summary,\n",
    "           \"cumulative_performance\": cumulative_performance,\n",
    "           \"target_level\": target_level,\n",
    "           \"emp_name\": emp_name\n",
    "       })\n",
    "       json_output_raw = response.content\n",
    "\n",
    "       json_output = _extract_json_from_llm_response(json_output_raw)\n",
    "\n",
    "       llm_parsed_data = json.loads(json_output)\n",
    "\n",
    "       score = llm_parsed_data.get(\"score\") \n",
    "       comment = llm_parsed_data.get(\"comment\") \n",
    "\n",
    "       if not isinstance(score, (int, float)) or not (0 <= score <= 100):\n",
    "           raise ValueError(f\"LLM 반환 점수 {score}가 유효하지 않습니다.\")\n",
    "       if not isinstance(comment, str) or not comment:\n",
    "           raise ValueError(f\"LLM 반환 코멘트 {comment}가 유효하지 않습니다.\")\n",
    "\n",
    "       return {\"score\": round(float(score), 2), \"comment\": comment}\n",
    "       \n",
    "   except json.JSONDecodeError as e:\n",
    "       print(f\"LLM 응답 JSON 파싱 오류: {e}. 원본 응답: '{json_output_raw}'. 파싱 시도 텍스트: '{json_output[:100]}...'\")\n",
    "       return {\"score\": 0.0, \"comment\": f\"AI 분석 실패: JSON 파싱 오류 - {json_output_raw[:100]}...\"}\n",
    "   except ValueError as e:\n",
    "       print(f\"LLM 응답 데이터 유효성 오류: {e}. 원본 응답: '{json_output_raw}'. 파싱 시도 텍스트: '{json_output[:100]}...'\")\n",
    "       return {\"score\": 0.0, \"comment\": f\"AI 분석 실패: 유효성 오류 - {json_output_raw[:100]}...\"}\n",
    "   except Exception as e:\n",
    "       print(f\"LLM 호출 중 예기치 않은 오류 발생: {e}. 원본 응답: '{json_output_raw}'\")\n",
    "       return {\"score\": 0.0, \"comment\": f\"AI 분석 실패: 예기치 않은 오류 - {str(e)[:100]}...\"}\n",
    "\n",
    "def call_llm_for_team_overall_analysis(all_team_kpis_results: List[Dict]) -> Dict:\n",
    "   print(f\"LLM Call (Team Overall Analysis): KPI {len(all_team_kpis_results)}개 기반 분석 요청.\")\n",
    "\n",
    "   kpi_details_str = \"\"\n",
    "   for kpi in all_team_kpis_results:\n",
    "       kpi_details_str += f\"- KPI: {kpi.get('kpi_name')} (ID: {kpi.get('team_kpi_id')})\\n\"\n",
    "       kpi_details_str += f\"  Description: {kpi.get('kpi_description')}\\n\"\n",
    "       if kpi.get('ai_kpi_overall_progress_rate') is not None:\n",
    "           kpi_details_str += f\"  AI 진행률: {kpi.get('ai_kpi_overall_progress_rate')}%\\n\"\n",
    "       if kpi.get('ai_kpi_analysis_comment'):\n",
    "           kpi_details_str += f\"  AI 코멘트: {kpi.get('ai_kpi_analysis_comment')}\\n\"\n",
    "       kpi_details_str += \"\\n\"\n",
    "\n",
    "   system_prompt = \"\"\"\n",
    "   당신은 SK 조직의 고위 경영진을 위한 팀 성과 분석 전문가입니다.\n",
    "   아래 제공된 팀의 KPI 정보, 설명, 그리고 AI가 분석한 각 KPI의 진행률 및 코멘트를 종합적으로 검토하여,\n",
    "   이 팀의 전반적인 목표 달성률을 추정하고, 팀 성과의 주요 특징과 개선점에 대한 분석 코멘트를 생성해주세요.\n",
    "\n",
    "   결과는 다음 JSON 형식으로만 응답해주세요. 불필요한 서문이나 추가 설명 없이 JSON만 반환해야 합니다.\n",
    "   \"\"\"\n",
    "\n",
    "   human_prompt = f\"\"\"\n",
    "   <팀 KPI 종합 정보>\n",
    "   {kpi_details_str}\n",
    "   </팀 KPI 종합 정보>\n",
    "\n",
    "   JSON 응답:\n",
    "   {{\n",
    "     \"overall_rate\": [팀 전체의 목표 달성률 추정 (0-100점)],\n",
    "     \"comment\": \"[팀 성과에 대한 전반적인 분석 코멘트]\"\n",
    "   }}\n",
    "   \"\"\"\n",
    "   \n",
    "   prompt = ChatPromptTemplate.from_messages([\n",
    "       SystemMessage(content=system_prompt),\n",
    "       HumanMessage(content=human_prompt)\n",
    "   ])\n",
    "   \n",
    "   chain = prompt | llm_client\n",
    "\n",
    "   try:\n",
    "       response: AIMessage = chain.invoke({\"all_team_kpis_results\": all_team_kpis_results})\n",
    "       json_output_raw = response.content\n",
    "       \n",
    "       json_output = _extract_json_from_llm_response(json_output_raw)\n",
    "       \n",
    "       llm_parsed_data = json.loads(json_output)\n",
    "       \n",
    "       overall_rate = llm_parsed_data.get(\"overall_rate\")\n",
    "       comment = llm_parsed_data.get(\"comment\")\n",
    "\n",
    "       if not isinstance(overall_rate, (int, float)) or not (0 <= overall_rate <= 100):\n",
    "           raise ValueError(f\"LLM 반환 전체 달성률 {overall_rate}가 유효하지 않습니다.\")\n",
    "       if not isinstance(comment, str) or not comment:\n",
    "           raise ValueError(f\"LLM 반환 코멘트 {comment}가 유효하지 않습니다.\")\n",
    "\n",
    "       return {\"overall_rate\": round(float(overall_rate), 2), \"comment\": comment}\n",
    "\n",
    "   except json.JSONDecodeError as e:\n",
    "       print(f\"LLM 응답 JSON 파싱 오류: {e}. 원본 응답: '{json_output_raw}'. 파싱 시도 텍스트: '{json_output[:100]}...'\")\n",
    "       return {\"overall_rate\": 0.0, \"comment\": f\"AI 분석 실패: JSON 파싱 오류 - {json_output_raw[:100]}...\"}\n",
    "   except ValueError as e:\n",
    "       print(f\"LLM 응답 데이터 유효성 오류: {e}. 원본 응답: '{json_output_raw}'. 파싱 시도 텍스트: '{json_output[:100]}...'\")\n",
    "       return {\"overall_rate\": 0.0, \"comment\": f\"AI 분석 실패: 유효성 오류 - {json_output_raw[:100]}...\"}\n",
    "   except Exception as e:\n",
    "       print(f\"LLM 호출 중 예기치 않은 오류 발생: {e}. 원본 응답: '{json_output_raw}'\")\n",
    "       return {\"overall_rate\": 0.0, \"comment\": f\"AI 분석 실패: 예기치 않은 오류 - {str(e)[:100]}...\"}\n",
    "\n",
    "\n",
    "def call_llm_for_kpi_relative_contribution(kpi_analysis_input: Dict) -> Dict:\n",
    "   kpi_goal = kpi_analysis_input.get(\"kpi_goal\", \"알 수 없는 목표\")\n",
    "   kpi_description = kpi_analysis_input.get(\"kpi_description\", \"\")\n",
    "   team_tasks = kpi_analysis_input.get(\"team_members_tasks\", [])\n",
    "   \n",
    "   print(f\"LLM Call (KPI Relative Contribution): '{kpi_goal[:30]}...' KPI 내 개인별 상대 기여도 분석 요청.\")\n",
    "   \n",
    "   actual_emp_nos_in_kpi = sorted(list(set(task.get('emp_no') for task in team_tasks if task.get('emp_no'))))\n",
    "\n",
    "   system_prompt = \"\"\"\n",
    "   당신은 팀 KPI 성과에 대한 개인별 기여도를 평가하는 전문가입니다.\n",
    "   아래는 특정 팀 KPI의 목표, 설명, 그리고 이 KPI에 기여한 팀원들의 Task 상세 내용 및 AI가 분석한 개별 Task 기여도 점수입니다.\n",
    "   \n",
    "   이 정보를 종합적으로 검토하여 다음을 수행하세요:\n",
    "   1. 이 KPI에 대한 각 개인의 **상대적인 기여도 점수 (총합 100%)**를 판단하세요.\n",
    "      - 반환하는 JSON의 `individual_relative_contributions_in_kpi` 딕셔너리에는 아래 <실제 팀원 사번 목록>에 있는 모든 사번에 대해 기여도를 포함해야 합니다.\n",
    "      - 각 개인의 기여도 점수(0-100점)는 소수점 두 자리까지 허용합니다.\n",
    "      - 어떤 팀원의 기여도가 0%이더라도 해당 사번과 0점을 명시적으로 포함해야 합니다.\n",
    "      - 모든 팀원의 기여도 합계가 100%가 되도록 조정해야 합니다.\n",
    "   2. KPI 전체의 진행 상황에 대한 간략한 분석 코멘트를 생성하세요.\n",
    "\n",
    "   평가 시 다음을 고려해야 합니다:\n",
    "   - 각 Task의 내용이 KPI 목표 달성에 얼마나 중요한가?\n",
    "   - 각 Task의 AI 기여도 점수는 어떤 의미인가? (개별 Task의 품질 및 중요도)\n",
    "   - 팀원 간 Task의 상호 의존성, 선행/후행 관계, 협업 기여도\n",
    "   - 특정 팀원이 여러 Task를 수행했거나, 더 중요한 Task를 수행했는가?\n",
    "   - 결과물 JSON에 불필요한 텍스트를 포함하지 마세요.\n",
    "   - 직원 이름을 언급할 때는 반드시 \"이름(사번)님\" 형태로 작성해주세요.\n",
    "\n",
    "\n",
    "   결과는 다음 JSON 형식으로만 응답해주세요:\n",
    "   \"\"\"\n",
    "\n",
    "   team_tasks_str = \"\"\n",
    "   for task in team_tasks:\n",
    "       emp_name = task.get('emp_name', '이름없음') \n",
    "       emp_no = task.get('emp_no', '사번없음')\n",
    "       team_tasks_str += f\"- 팀원: {emp_name}({emp_no})님, Task: {task.get('task_name')}\\n\" \n",
    "       team_tasks_str += f\"  요약: {task.get('task_summary')}\\n\"\n",
    "       if task.get('ai_contribution_score_from_individual_analysis') is not None:\n",
    "           team_tasks_str += f\"  개별 AI 기여도 점수 (참고용): {task.get('ai_contribution_score_from_individual_analysis')}점\\n\"\n",
    "       team_tasks_str += \"\\n\"\n",
    "\n",
    "\n",
    "   individual_contributions_json_example = \",\\n\".join([f'\"{emp_no}\": [상대 기여도 (0-100점)]' for emp_no in actual_emp_nos_in_kpi])\n",
    "   if not individual_contributions_json_example:\n",
    "       individual_contributions_json_example = '\"EMP_NO_X\": [상대 기여도 (0-100점)]'\n",
    "\n",
    "   human_prompt = f\"\"\"\n",
    "   <팀 KPI 목표>\n",
    "   {kpi_goal}\n",
    "   </팀 KPI 목표>\n",
    "   <팀 KPI 설명>\n",
    "   {kpi_description}\n",
    "   </팀 KPI 설명>\n",
    "   <팀원 Task 정보>\n",
    "   {team_tasks_str}\n",
    "   </팀원 Task 정보>\n",
    "   <실제 팀원 사번 목록>\n",
    "   {', '.join(actual_emp_nos_in_kpi) if actual_emp_nos_in_kpi else '없음'}\n",
    "   </실제 팀원 사번 목록>\n",
    "\n",
    "   JSON 응답:\n",
    "   {{\n",
    "     \"kpi_overall_rate\": [KPI 전체의 진행 상황에 대한 점수 (0-100점)],\n",
    "     \"kpi_analysis_comment\": \"[KPI 전체 진행 상황에 대한 분석 코멘트]\",\n",
    "     \"individual_relative_contributions_in_kpi\": {{\n",
    "       {individual_contributions_json_example}\n",
    "     }}\n",
    "   }}\n",
    "   \"\"\"\n",
    "   \n",
    "   prompt = ChatPromptTemplate.from_messages([\n",
    "       SystemMessage(content=system_prompt),\n",
    "       HumanMessage(content=human_prompt)\n",
    "   ])\n",
    "   \n",
    "   chain = prompt | llm_client\n",
    "\n",
    "   try:\n",
    "       response: AIMessage = chain.invoke({\"kpi_analysis_input\": kpi_analysis_input})\n",
    "       json_output_raw = response.content\n",
    "       json_output = _extract_json_from_llm_response(json_output_raw)\n",
    "       \n",
    "       llm_parsed_data = json.loads(json_output)\n",
    "       \n",
    "       kpi_overall_rate = llm_parsed_data.get(\"kpi_overall_rate\")\n",
    "       kpi_analysis_comment = llm_parsed_data.get(\"kpi_analysis_comment\")\n",
    "       individual_relative_contributions_raw_from_llm = llm_parsed_data.get(\"individual_relative_contributions_in_kpi\")\n",
    "\n",
    "       if not isinstance(kpi_overall_rate, (int, float)) or not (0 <= kpi_overall_rate <= 100):\n",
    "           raise ValueError(f\"LLM 반환 KPI 전체 진행률 {kpi_overall_rate}가 유효하지 않습니다.\")\n",
    "       if not isinstance(kpi_analysis_comment, str) or not kpi_analysis_comment:\n",
    "           raise ValueError(f\"LLM 반환 KPI 분석 코멘트 {kpi_analysis_comment}가 유효하지 않습니다.\")\n",
    "       if not isinstance(individual_relative_contributions_raw_from_llm, dict):\n",
    "           raise ValueError(f\"LLM 반환 개인 상대 기여도 형식 {individual_relative_contributions_raw_from_llm}가 유효하지 않습니다.\")\n",
    "       \n",
    "       # --- 파싱 로직 보강: LLM이 반환한 사번 외의 사번 처리 및 합계 검증 ---\n",
    "       final_relative_contributions = {}\n",
    "       for emp_no in actual_emp_nos_in_kpi:\n",
    "           final_relative_contributions[emp_no] = 0.0\n",
    "       \n",
    "       for emp_no_from_llm, score in individual_relative_contributions_raw_from_llm.items():\n",
    "           if emp_no_from_llm in final_relative_contributions and isinstance(score, (int, float)):\n",
    "               final_relative_contributions[emp_no_from_llm] = round(float(score), 2)\n",
    "           else:\n",
    "               print(f\"Warning: LLM이 예상치 못한 사번 '{emp_no_from_llm}'를 반환했거나 점수가 유효하지 않아 무시됩니다. 점수: {score}\")\n",
    "\n",
    "       total_relative_sum = sum(final_relative_contributions.values())\n",
    "       if total_relative_sum > 0 and not (99.9 <= total_relative_sum <= 100.1):\n",
    "           print(f\"Warning: 개인 상대 기여도 합계가 100%와 다릅니다: {total_relative_sum}%. 재조정 시도.\")\n",
    "           adjustment_factor = 100.0 / total_relative_sum if total_relative_sum > 0 else 1.0\n",
    "           adjusted_contributions = {k: round(v * adjustment_factor, 2) for k, v in final_relative_contributions.items()}\n",
    "           final_relative_contributions = adjusted_contributions\n",
    "           print(f\"재조정된 기여도: {final_relative_contributions}\")\n",
    "           \n",
    "       return {\n",
    "           \"kpi_overall_rate\": round(float(kpi_overall_rate), 2),\n",
    "           \"kpi_analysis_comment\": kpi_analysis_comment,\n",
    "           \"individual_relative_contributions_in_kpi\": final_relative_contributions\n",
    "       }\n",
    "       \n",
    "   except json.JSONDecodeError as e:\n",
    "       print(f\"LLM 응답 JSON 파싱 오류: {e}. 원본 응답: '{json_output_raw}'. 파싱 시도 텍스트: '{json_output[:100]}...'\")\n",
    "       return {\n",
    "           \"kpi_overall_rate\": 0.0,\n",
    "           \"kpi_analysis_comment\": f\"AI 분석 실패: JSON 파싱 오류 - {json_output_raw[:100]}...\",\n",
    "           \"individual_relative_contributions_in_kpi\": {}\n",
    "       }\n",
    "   except ValueError as e:\n",
    "       print(f\"LLM 응답 데이터 유효성 오류: {e}. 원본 응답: '{json_output_raw}'. 파싱 시도 텍스트: '{json_output[:100]}...'\")\n",
    "       return {\n",
    "           \"kpi_overall_rate\": 0.0,\n",
    "           \"kpi_analysis_comment\": f\"AI 분석 실패: 유효성 오류 - {json_output_raw[:100]}...\",\n",
    "           \"individual_relative_contributions_in_kpi\": {}\n",
    "       }\n",
    "   except Exception as e:\n",
    "       print(f\"LLM 호출 중 예기치 않은 오류 발생: {e}. 원본 응답: '{json_output_raw}'\")\n",
    "       return {\n",
    "           \"kpi_overall_rate\": 0.0,\n",
    "           \"kpi_analysis_comment\": f\"AI 분석 실패: 예기치 않은 오류 - {str(e)[:100]}...\",\n",
    "           \"individual_relative_contributions_in_kpi\": {}\n",
    "       }\n",
    "   \n",
    "\n",
    "\n",
    "def call_llm_for_individual_contribution_reason_comment(\n",
    "   task_info: Dict, \n",
    "   adjusted_contribution_score: float, \n",
    "   kpi_goal: str, \n",
    "   kpi_overall_comment: str) -> Dict:\n",
    "   \n",
    "   emp_name = task_info.get(\"emp_name\", \"이름 없음\")\n",
    "   emp_no = task_info.get(\"emp_no\", \"사번 없음\")\n",
    "   task_name = task_info.get(\"task_name\", \"알 수 없는 Task\")\n",
    "   \n",
    "   # --- 수정: 누적 요약 정보 우선 사용 ---\n",
    "   task_summary_text = task_info.get(\"cumulative_task_summary\", \n",
    "                                    task_info.get(\"task_summary\", \"상세 내용 없음\"))\n",
    "\n",
    "   print(f\"LLM Call (Individual Contribution Reason): '{emp_name} ({emp_no})'의 '{task_name[:30]}...' Task 근거 요청.\")\n",
    "\n",
    "   system_prompt = \"\"\"\n",
    "   당신은 SK 조직의 성과 평가 전문가이자 명확한 근거를 제시하는 분석가입니다.\n",
    "   아래 제공된 개인의 특정 Task 상세 내용, 이 Task가 속한 KPI의 목표, 그리고 팀 전체에 대한 KPI 분석 코멘트를 종합적으로 고려하여,\n",
    "   이 Task의 최종 조정된 기여도 점수(KPI 내 상대적 기여도)가 왜 그렇게 산정되었는지에 대한 구체적이고 복합적인 근거 코멘트를 작성해주세요.\n",
    "\n",
    "   코멘트는 다음 요소를 포함해야 합니다:\n",
    "   - Task 자체의 내용과 중요도 (전체 분기별 활동 기반)\n",
    "   - LLM이 판단한 KPI 내 상대적 기여도 점수 (제시된 점수 활용)\n",
    "   - 이 Task가 팀 KPI 목표 달성에 어떻게 기여했는지 (KPI 목표, 전체 KPI 코멘트 기반)\n",
    "   - Task 간의 상호 관계나 협업 등의 맥락이 기여도에 미친 영향 (제공된 정보 내에서 추론)\n",
    "   - 직원 이름을 언급할 때는 반드시 \"이름(사번)님\" 형태로 작성해주세요.\n",
    "\n",
    "   결과는 다음 JSON 형식으로만 응답해주세요. 불필요한 서문이나 추가 설명 없이 JSON만 반환해야 합니다.\n",
    "   \"\"\"\n",
    "\n",
    "   human_prompt = f\"\"\"\n",
    "   <Task 상세 정보>\n",
    "   이름: {emp_name}\n",
    "   사번: {emp_no}\n",
    "   Task 이름: {task_name}\n",
    "   Task 연간 누적 활동/성과: {task_summary_text}\n",
    "   조정된 기여도 점수: {adjusted_contribution_score}점\n",
    "   </Task 상세 정보>\n",
    "\n",
    "   <KPI 정보>\n",
    "   KPI 목표: {kpi_goal}\n",
    "   KPI 전체 분석 코멘트: {kpi_overall_comment}\n",
    "   </KPI 정보>\n",
    "\n",
    "   JSON 응답:\n",
    "   {{\n",
    "     \"comment_reason\": \"[{emp_name}({emp_no})님의 해당 Task에 대한 구체적 근거 코멘트]\"\n",
    "   }}\n",
    "   \"\"\"\n",
    "\n",
    "   prompt = ChatPromptTemplate.from_messages([\n",
    "       SystemMessage(content=system_prompt),\n",
    "       HumanMessage(content=human_prompt)\n",
    "   ])\n",
    "   \n",
    "   chain = prompt | llm_client\n",
    "\n",
    "   try:\n",
    "       response: AIMessage = chain.invoke({\n",
    "           \"task_info\": task_info, \n",
    "           \"adjusted_contribution_score\": adjusted_contribution_score, \n",
    "           \"kpi_goal\": kpi_goal, \n",
    "           \"kpi_overall_comment\": kpi_overall_comment\n",
    "       })\n",
    "       json_output_raw = response.content\n",
    "       json_output = _extract_json_from_llm_response(json_output_raw)\n",
    "       llm_parsed_data = json.loads(json_output)\n",
    "       \n",
    "       comment_reason = llm_parsed_data.get(\"comment_reason\")\n",
    "\n",
    "       if not isinstance(comment_reason, str) or not comment_reason:\n",
    "           raise ValueError(f\"LLM 반환 근거 코멘트 {comment_reason}가 유효하지 않습니다.\")\n",
    "\n",
    "       return {\"comment\": comment_reason}\n",
    "       \n",
    "   except json.JSONDecodeError as e:\n",
    "       print(f\"LLM 응답 JSON 파싱 오류: {e}. 원본 응답: '{json_output_raw}'. 파싱 시도 텍스트: '{json_output[:100]}...'\")\n",
    "       return {\"comment\": f\"AI 근거 생성 실패: JSON 파싱 오류 - {json_output_raw[:100]}...\"}\n",
    "   except ValueError as e:\n",
    "       print(f\"LLM 응답 데이터 유효성 오류: {e}. 응답: {json_output}\")\n",
    "       return {\"comment\": f\"AI 근거 생성 실패: 유효성 오류 - {json_output[:100]}...\"}\n",
    "   except Exception as e:\n",
    "        print(f\"LLM 호출 중 예기치 않은 오류 발생: {e}. 원본 응답: '{json_output_raw}'\")\n",
    "        return {\"comment\": f\"AI 근거 생성 실패: 예기치 않은 오류 - {str(e)[:100]}...\"}\n",
    "\n",
    "\n",
    "# --- 서브모듈 함수 정의 ---\n",
    "\n",
    "# 1. 데이터 수집 서브모듈\n",
    "def data_collection_submodule(state: Module2AgentState) -> Module2AgentState:\n",
    "   messages = state.get(\"messages\", []) + [HumanMessage(content=\"모듈 2: 데이터 수집 ID 초기화 완료\")] \n",
    "   return {\"messages\": messages}\n",
    "\n",
    "\n",
    "# 2. 개인 기여도 계산 서브모듈\n",
    "def calculate_individual_contribution_submodule(state: Module2AgentState) -> Module2AgentState:\n",
    "    report_type = state[\"report_type\"] \n",
    "    target_task_ids = state[\"target_task_ids\"]\n",
    "    period_id = state[\"period_id\"]\n",
    "    \n",
    "    updated_task_summary_ids_list = []\n",
    "\n",
    "    for task_id in target_task_ids:\n",
    "        task_summaries = fetch_task_summaries_by_task_and_period(task_id, period_id)\n",
    "        \n",
    "        if not task_summaries:\n",
    "            print(f\"Warning: No task summaries found for task_id {task_id}.\")\n",
    "            continue\n",
    "        \n",
    "        # 🔧 Task당 1번만 LLM 호출 (전체 누적 데이터 기반)\n",
    "        latest_task_data = task_summaries[-1]  # 가장 최신 데이터\n",
    "        combined_summary = \"\\n\".join([f\"Q{summary['period_id']}: {summary['task_summary']}\" for summary in task_summaries])\n",
    "        cumulative_performance = latest_task_data.get('task_performance', '')\n",
    "        target_level = latest_task_data.get('target_level', '')\n",
    "        emp_name = latest_task_data.get('emp_name', '')\n",
    "        \n",
    "        # 🔧 1번만 호출 (전체 분기 데이터 기반)\n",
    "        if report_type == \"quarterly\":\n",
    "            llm_results = call_llm_for_cumulative_task_contribution(\n",
    "                task_id, combined_summary, cumulative_performance, target_level, emp_name\n",
    "            )\n",
    "            base_update_data = {\n",
    "                \"ai_contribution_score\": llm_results.get(\"score\"), \n",
    "                \"ai_analysis_comment_task\": llm_results.get(\"comment\") \n",
    "            }\n",
    "        \n",
    "        elif report_type == \"annual\":\n",
    "            contribution_results = call_llm_for_cumulative_task_contribution(\n",
    "                task_id, combined_summary, cumulative_performance, target_level, emp_name\n",
    "            )\n",
    "            \n",
    "            if target_level and cumulative_performance:\n",
    "                grade_definitions = fetch_grade_definitions_from_db()\n",
    "                achievement_results = call_llm_for_task_achievement(\n",
    "                    target_level, cumulative_performance, grade_definitions\n",
    "                )\n",
    "                \n",
    "                base_update_data = {\n",
    "                    \"ai_contribution_score\": contribution_results.get(\"score\"), \n",
    "                    \"ai_achievement_rate\": achievement_results.get(\"rate\"), \n",
    "                    \"ai_assessed_grade\": achievement_results.get(\"grade\"), \n",
    "                    \"ai_analysis_comment_task\": achievement_results.get(\"analysis\") \n",
    "                }\n",
    "            else:\n",
    "                base_update_data = {\n",
    "                    \"ai_contribution_score\": contribution_results.get(\"score\"), \n",
    "                    \"ai_analysis_comment_task\": contribution_results.get(\"comment\") \n",
    "                }\n",
    "\n",
    "        # 🔧 각 분기별로 비례 배분하여 저장\n",
    "        for task_summary in task_summaries:\n",
    "            task_summary_id = task_summary.get('task_summary_Id')\n",
    "            current_period = task_summary.get('period_id')\n",
    "            \n",
    "            # 분기별 비례 배분\n",
    "            period_weight = current_period / period_id\n",
    "            \n",
    "            update_data = base_update_data.copy()\n",
    "            if \"ai_contribution_score\" in update_data:\n",
    "                update_data[\"ai_contribution_score\"] = round(base_update_data[\"ai_contribution_score\"] * period_weight, 2)\n",
    "            \n",
    "            if update_task_summary_ai_results_in_db(task_summary_id, update_data):\n",
    "                updated_task_summary_ids_list.append(task_summary_id)\n",
    "\n",
    "    messages = state.get(\"messages\", []) + [HumanMessage(content=f\"모듈 2: 개인 기여도 계산 완료 ({len(updated_task_summary_ids_list)}건)\")]\n",
    "    return {\"messages\": messages, \"updated_task_ids\": updated_task_summary_ids_list}\n",
    "\n",
    "\n",
    "# 3. 팀 목표 분석 서브모듈 (수정: KPI 내 개인 상대 기여도 계산 및 LLM 요청 후 tasks 업데이트)\n",
    "def analyze_team_goals_submodule(state: Module2AgentState) -> Module2AgentState:\n",
    "    report_type = state[\"report_type\"] \n",
    "    target_team_kpi_ids = state[\"target_team_kpi_ids\"] \n",
    "    period_id = state[\"period_id\"] \n",
    "\n",
    "    updated_team_kpi_ids_list = [] \n",
    "    kpi_individual_relative_contributions_for_state = [] \n",
    "\n",
    "    for team_kpi_id in target_team_kpi_ids: \n",
    "        kpi_data = fetch_kpi_data_by_id(team_kpi_id) \n",
    "        if not kpi_data: \n",
    "            print(f\"Warning: Team KPI data not found for team_kpi_id {team_kpi_id}.\") \n",
    "            continue \n",
    "\n",
    "        tasks_in_this_kpi = fetch_tasks_for_kpi(team_kpi_id, period_id) \n",
    "        \n",
    "        # 🔧 최신 분기 데이터만 사용하여 중복 제거\n",
    "        latest_period_tasks = [\n",
    "            task for task in tasks_in_this_kpi \n",
    "            if task.get(\"period_id\") == period_id\n",
    "        ]\n",
    "        \n",
    "        llm_input_for_kpi_analysis = {\n",
    "            \"kpi_goal\": kpi_data.get(\"kpi_name\"), \n",
    "            \"kpi_description\": kpi_data.get(\"kpi_description\"), \n",
    "            \"team_members_tasks\": [\n",
    "                {\n",
    "                    \"emp_no\": task.get(\"emp_no\"), \n",
    "                    \"task_id\": task.get(\"task_id\"), \n",
    "                    \"task_name\": task.get(\"task_name\"), \n",
    "                    \"task_summary\": task.get(\"task_summary\"), \n",
    "                    \"ai_contribution_score_from_individual_analysis\": task.get(\"ai_contribution_score\") \n",
    "                } for task in latest_period_tasks  # 🔧 최신 분기만 사용\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        llm_kpi_analysis_results = call_llm_for_kpi_relative_contribution(llm_input_for_kpi_analysis) \n",
    "\n",
    "        update_data_kpi = { \n",
    "            \"ai_kpi_progress_rate\": llm_kpi_analysis_results.get(\"kpi_overall_rate\"),\n",
    "            \"ai_kpi_analysis_comment\": llm_kpi_analysis_results.get(\"kpi_analysis_comment\") \n",
    "        }\n",
    "\n",
    "        if update_team_kpi_ai_results_in_db(team_kpi_id, update_data_kpi): \n",
    "            updated_team_kpi_ids_list.append(team_kpi_id) \n",
    "            \n",
    "            if \"individual_relative_contributions_in_kpi\" in llm_kpi_analysis_results: \n",
    "                relative_contributions_by_emp = llm_kpi_analysis_results[\"individual_relative_contributions_in_kpi\"]\n",
    "                kpi_individual_relative_contributions_for_state.append({ \n",
    "                    \"team_kpi_id\": team_kpi_id,\n",
    "                    \"relative_contributions\": relative_contributions_by_emp\n",
    "                }) \n",
    "\n",
    "                # 🔧 최신 분기 데이터 기준으로 고유한 task_id 목록 추출\n",
    "                unique_task_ids = list(set(task.get(\"task_id\") for task in latest_period_tasks))\n",
    "\n",
    "                for task_id in unique_task_ids:\n",
    "                    emp_no_for_this_task = None\n",
    "                    \n",
    "                    # 해당 task_id의 모든 분기 데이터 수집 (전체 tasks_in_this_kpi에서)\n",
    "                    task_summaries_for_this_task = []\n",
    "                    for task in tasks_in_this_kpi:\n",
    "                        if task.get(\"task_id\") == task_id:\n",
    "                            emp_no_for_this_task = task.get(\"emp_no\")\n",
    "                            task_summaries_for_this_task.append(task)\n",
    "                    \n",
    "                    if emp_no_for_this_task in relative_contributions_by_emp:\n",
    "                        new_contribution_score = relative_contributions_by_emp[emp_no_for_this_task]\n",
    "                        \n",
    "                        # 🔧 Task당 1번만 근거 생성 (전체 분기 데이터 기반)\n",
    "                        cumulative_data = fetch_cumulative_task_data_for_comment(task_id, period_id)\n",
    "                        \n",
    "                        if cumulative_data:\n",
    "                            # 🔧 1번만 LLM 호출 (전체 연간 활동 기반)\n",
    "                            reason_llm_results = call_llm_for_individual_contribution_reason_comment(\n",
    "                                cumulative_data,\n",
    "                                float(new_contribution_score),\n",
    "                                kpi_data.get('kpi_name', ''),\n",
    "                                llm_kpi_analysis_results.get('kpi_analysis_comment', '')\n",
    "                            )\n",
    "                            adjusted_comment = reason_llm_results.get(\"comment\", f\"AI 근거 생성 실패: {emp_no_for_this_task}의 Task {task_id}에 대한 근거를 생성할 수 없습니다.\")\n",
    "                            \n",
    "                            # 🔧 모든 분기에 동일한 근거 저장하되 기여도는 분기별 비례 배분\n",
    "                            for task_summary in task_summaries_for_this_task:\n",
    "                                task_summary_id = task_summary.get(\"task_summary_Id\")\n",
    "                                current_period = task_summary.get(\"period_id\")\n",
    "                                \n",
    "                                if current_period is None:\n",
    "                                    print(f\"Warning: current_period is None for task_summary_id {task_summary_id}\")\n",
    "                                    continue\n",
    "                                \n",
    "                                # 해당 분기까지의 기여도 계산 (최종값에서 비례 배분)\n",
    "                                period_weight = current_period / period_id\n",
    "                                period_contribution = new_contribution_score * period_weight\n",
    "                                \n",
    "                                update_data_task_summary = {\n",
    "                                    \"ai_contribution_score\": round(period_contribution, 2),\n",
    "                                    \"ai_analysis_comment_task\": adjusted_comment\n",
    "                                }\n",
    "                                \n",
    "                                if not update_task_summary_ai_results_in_db(task_summary_id, update_data_task_summary):\n",
    "                                    print(f\"Warning: Failed to update task_summary_Id {task_summary_id} (task_id: {task_id}, period: {current_period})\")\n",
    "                        else:\n",
    "                            print(f\"Warning: No cumulative data found for task_id {task_id}\")\n",
    "                    else:\n",
    "                        print(f\"Warning: Emp_no {emp_no_for_this_task} from task_id {task_id} not found in LLM's relative contributions for KPI {team_kpi_id}.\")\n",
    "\n",
    "        else: \n",
    "            print(f\"Failed to update AI results for team_kpi_id: {team_kpi_id}\") \n",
    "\n",
    "    messages = state.get(\"messages\", []) + [HumanMessage(content=f\"모듈 2: 팀 목표 분석 및 DB 업데이트 완료 ({len(updated_team_kpi_ids_list)}건)\")] \n",
    "    \n",
    "    return {\"messages\": messages, \"updated_team_kpi_ids\": updated_team_kpi_ids_list,\n",
    "            \"kpi_individual_relative_contributions\": kpi_individual_relative_contributions_for_state}\n",
    "\n",
    "\n",
    "# 4. 모듈 2 관련 레포트 테이블 데이터 생성/업데이트 서브모듈\n",
    "def generate_module2_report_data_submodule(state: Module2AgentState) -> Module2AgentState:\n",
    "    report_type = state[\"report_type\"] \n",
    "    team_id = state[\"team_id\"] \n",
    "    period_id = state[\"period_id\"] \n",
    "    \n",
    "    kpi_individual_relative_contributions = state.get(\"kpi_individual_relative_contributions\", []) \n",
    "    \n",
    "    updated_ids_for_state = {} \n",
    "\n",
    "    # 개인의 팀 전체 기여도 계산 (KPI별 상대 기여도 기반)\n",
    "    emp_overall_relative_contributions = {} \n",
    "    \n",
    "    for kpi_result in kpi_individual_relative_contributions: \n",
    "        for emp_no, relative_score in kpi_result[\"relative_contributions\"].items(): \n",
    "            if emp_no not in emp_overall_relative_contributions: \n",
    "                emp_overall_relative_contributions[emp_no] = 0 \n",
    "            emp_overall_relative_contributions[emp_no] += relative_score \n",
    "\n",
    "    # --- 팀 전체 기여도 합계 100%로 정규화 ---\n",
    "    total_sum_of_relative_contributions = sum(emp_overall_relative_contributions.values())\n",
    "    if total_sum_of_relative_contributions > 0:\n",
    "        adjustment_factor = 100.0 / total_sum_of_relative_contributions\n",
    "        for emp_no, score in emp_overall_relative_contributions.items():\n",
    "            emp_overall_relative_contributions[emp_no] = round(score * adjustment_factor, 2)\n",
    "    # ----------------------------------------\n",
    "\n",
    "    # 모든 개인 Task 결과는 여전히 필요 \n",
    "    all_individual_task_results_raw = []\n",
    "    for task_id in state[\"target_task_ids\"]:\n",
    "        task_summaries = fetch_task_summaries_by_task_and_period(task_id, period_id)\n",
    "        all_individual_task_results_raw.extend(task_summaries)\n",
    "\n",
    "    # 개인용 분기별 피드백 레포트 (feedback_reports)\n",
    "    if report_type == \"quarterly\": \n",
    "        # 1. 해당 팀의 모든 emp_no 조회 (피드백 레포트는 팀원용)\n",
    "        all_team_members_in_db = fetch_employees_by_team_id(team_id)\n",
    "\n",
    "        for member_info in all_team_members_in_db:\n",
    "            emp_no_current_member = member_info[\"emp_no\"]\n",
    "            emp_name_current_member = member_info[\"emp_name\"]\n",
    "\n",
    "            # 팀장(MANAGER) 역할은 피드백 레포트를 직접 생성하지 않으므로 건너뜁니다.\n",
    "            if member_info.get(\"role\") == \"MANAGER\": \n",
    "                print(f\"Info: Skipping feedback_reports for manager {emp_no_current_member}.\")\n",
    "                continue\n",
    "\n",
    "            # 해당 팀원에게 해당하는 Task Summaries 필터링\n",
    "            individual_tasks_for_report = [\n",
    "                task for task in all_individual_task_results_raw \n",
    "                if task.get(\"emp_no\") == emp_no_current_member and task.get(\"period_id\") <= period_id \n",
    "            ]\n",
    "\n",
    "            if not individual_tasks_for_report: \n",
    "                print(f\"Warning: No individual tasks found for emp_no {emp_no_current_member} in period {period_id}. Skipping feedback_reports save for this member.\") \n",
    "                continue \n",
    "\n",
    "            # LLM 호출 시 emp_name, emp_no 전달\n",
    "            individual_overall_results = call_llm_for_overall_contribution_summary(\n",
    "                individual_tasks_for_report, emp_name_current_member, emp_no_current_member\n",
    "            ) \n",
    "            calculated_individual_quarterly_contribution = emp_overall_relative_contributions.get(emp_no_current_member, 0) \n",
    "\n",
    "            team_evaluation_id_for_report = state.get(\"team_evaluation_id\") \n",
    "            if team_evaluation_id_for_report is None: \n",
    "                print(f\"Warning: team_evaluation_id for team_id={team_id}, period_id={period_id} is missing in state. Cannot save feedback_reports for {emp_no_current_member}. (앞단 Agent에서 생성 필요)\") \n",
    "            else: \n",
    "                actual_team_eval_id_in_db = fetch_team_evaluation_id_by_team_and_period(team_id, period_id) \n",
    "                if actual_team_eval_id_in_db != team_evaluation_id_for_report: \n",
    "                     print(f\"Warning: team_evaluation_id {team_evaluation_id_for_report} from state does not match existing ID in DB for team={team_id}, period={period_id}. Skipping feedback_reports save for {emp_no_current_member}.\") \n",
    "                else: \n",
    "                    # --- INSERT 또는 UPDATE 로직 (ON DUPLICATE KEY UPDATE 사용) ---\n",
    "                    feedback_report_id = save_feedback_report_module2_results_to_db(\n",
    "                        emp_no_current_member, team_evaluation_id_for_report, \n",
    "                        {\n",
    "                            \"contribution_rate\": int(calculated_individual_quarterly_contribution),  # 🔧 컬럼명 변경 및 int 변환\n",
    "                            \"ai_overall_contribution_summary_comment\": individual_overall_results.get(\"comment\"),\n",
    "                            \"ai_peer_talk_summary\": \"모듈 2에서는 생성하지 않음\"  # 🔧 새 컬럼 추가 (빈 값)\n",
    "                        }\n",
    "                    )\n",
    "                    updated_ids_for_state[\"feedback_report_id\"] = feedback_report_id \n",
    "                    messages = state.get(\"messages\", []) + [HumanMessage(content=f\"모듈 2: 개인 {emp_no_current_member} 분기별 레포트 내용 생성/업데이트 및 feedback_reports 저장 완료 (ID: {feedback_report_id})\")] \n",
    "\n",
    "    # 팀장용 분기별/연말 팀 전체 평가 레포트 (team_evaluations)\n",
    "    team_evaluation_id = state.get(\"team_evaluation_id\") \n",
    "    if team_evaluation_id is None: \n",
    "        print(f\"Warning: team_evaluation_id for team_id={team_id}, period_id={period_id} is missing in state. Cannot update team_evaluations. (앞단 Agent에서 생성 필요)\") \n",
    "    else: \n",
    "        actual_team_eval_id_in_db = fetch_team_evaluation_id_by_team_and_period(team_id, period_id) \n",
    "        if actual_team_eval_id_in_db != team_evaluation_id: \n",
    "             print(f\"Warning: team_evaluation_id {team_evaluation_id} from state does not match existing ID in DB for team={team_id}, period={period_id}. Skipping team_evaluations update.\") \n",
    "        else: \n",
    "            all_team_kpis_results = [fetch_kpi_data_by_id(kpi_id) for kpi_id in state[\"target_team_kpi_ids\"] if fetch_kpi_data_by_id(kpi_id)] \n",
    "            team_overall_results = call_llm_for_team_overall_analysis(all_team_kpis_results) \n",
    "            \n",
    "            update_data = {\n",
    "                \"average_achievement_rate\": int(team_overall_results.get(\"overall_rate\")),  # 🔧 컬럼명 변경 및 int 변환\n",
    "                \"ai_team_overall_analysis_comment\": team_overall_results.get(\"comment\") \n",
    "            }\n",
    "            update_team_evaluations_module2_results_in_db(team_evaluation_id, update_data) \n",
    "            updated_ids_for_state[\"team_evaluation_id\"] = team_evaluation_id \n",
    "            messages = state.get(\"messages\", []) + [HumanMessage(content=f\"모듈 2: 팀 전체 분석 코멘트 생성 및 team_evaluations 업데이트 완료 (ID: {team_evaluation_id})\")] \n",
    "\n",
    "    # 개인용 연말 최종 평가 레포트 (final_evaluation_reports)\n",
    "    if report_type == \"annual\": \n",
    "        all_team_members_in_db = fetch_employees_by_team_id(team_id)\n",
    "\n",
    "        for member_info in all_team_members_in_db:\n",
    "            emp_no_current_member = member_info[\"emp_no\"]\n",
    "            emp_name_current_member = member_info[\"emp_name\"]\n",
    "\n",
    "            # 팀장(MANAGER) 역할은 최종 평가 레포트의 직접 대상이 아니므로 건너뜁니다.\n",
    "            if member_info.get(\"role\") == \"MANAGER\": \n",
    "                print(f\"Info: Skipping final_evaluation_reports for manager {emp_no_current_member}.\")\n",
    "                continue\n",
    "\n",
    "            # 해당 팀원에게 해당하는 Task Summaries 필터링\n",
    "            individual_tasks_for_annual_report = [\n",
    "                task for task in all_individual_task_results_raw \n",
    "                if task.get(\"emp_no\") == emp_no_current_member and task.get(\"period_id\") <= period_id\n",
    "            ]\n",
    "            if not individual_tasks_for_annual_report: \n",
    "                print(f\"Warning: No individual tasks found for emp_no {emp_no_current_member} in period {period_id}. Skipping final_evaluation_reports save for this member.\") \n",
    "                continue \n",
    "\n",
    "            # LLM 호출 시 emp_name, emp_no 전달\n",
    "            annual_individual_summary_results = call_llm_for_overall_contribution_summary(\n",
    "                individual_tasks_for_annual_report, emp_name_current_member, emp_no_current_member\n",
    "            ) \n",
    "            \n",
    "            calculated_annual_individual_total_contribution = emp_overall_relative_contributions.get(emp_no_current_member, 0) \n",
    "            \n",
    "            final_team_evaluation_id_example = state.get(\"team_evaluation_id\") \n",
    "            if final_team_evaluation_id_example is None: \n",
    "                print(f\"Warning: team_evaluation_id for team_id={team_id}, period_id={period_id} is missing in state. Cannot save final_evaluation_reports for {emp_no_current_member}. (앞단 Agent에서 생성 필요)\") \n",
    "            else: \n",
    "                actual_team_eval_id_in_db = fetch_team_evaluation_id_by_team_and_period(team_id, period_id) \n",
    "                if actual_team_eval_id_in_db != final_team_evaluation_id_example: \n",
    "                     print(f\"Warning: team_evaluation_id {final_team_evaluation_id_example} from state does not match existing ID in DB for team={team_id}, period={period_id}. Skipping final_evaluation_reports save for {emp_no_current_member}.\") \n",
    "                else: \n",
    "                    final_report_id = save_final_evaluation_report_module2_results_to_db(\n",
    "                        emp_no_current_member, final_team_evaluation_id_example, \n",
    "                        {\n",
    "                            \"contribution_rate\": int(calculated_annual_individual_total_contribution),  # 🔧 컬럼명 변경 및 int 변환\n",
    "                            \"ai_annual_achievement_rate\": int(annual_individual_summary_results.get(\"average_rate\")),  # 🔧 int 변환\n",
    "                            \"ai_annual_performance_summary_comment\": annual_individual_summary_results.get(\"comment\"),\n",
    "                            \"ai_annual_peer_talk_summary\": \"모듈 2에서는 생성하지 않음\"  # 🔧 새 컬럼 추가 (빈 값)\n",
    "                        }\n",
    "                    )\n",
    "                    updated_ids_for_state[\"final_report_id\"] = final_report_id \n",
    "                    messages = state.get(\"messages\", []) + [HumanMessage(content=f\"모듈 2: 개인 {emp_no_current_member} 연말 최종 평가 레포트 내용 생성 및 final_evaluation_reports 저장 완료 (ID: {final_report_id})\")] \n",
    "\n",
    "    # 🔧 temp_evaluations 관련 코드 전체 삭제 (ai_annual_key_performance_contribution_summary 저장 안함)\n",
    "\n",
    "    return {\"messages\": messages, **updated_ids_for_state}\n",
    "\n",
    "# 5. 포맷터 서브모듈\n",
    "def formatter_submodule(state: Module2AgentState) -> Module2AgentState:\n",
    "   messages = state.get(\"messages\", []) + [HumanMessage(content=\"모듈 2: 포맷팅 완료\")]\n",
    "   return {\"messages\": messages}\n",
    "\n",
    "# --- LangGraph Workflow 구성 및 컴파일 ---\n",
    "# 모듈 2의 워크플로우 정의\n",
    "module2_workflow = StateGraph(Module2AgentState)\n",
    "\n",
    "# 노드 추가 (각 서브모듈 함수를 노드로 등록)\n",
    "module2_workflow.add_node(\"data_collection\", data_collection_submodule)\n",
    "module2_workflow.add_node(\"calculate_individual_contribution\", calculate_individual_contribution_submodule)\n",
    "module2_workflow.add_node(\"analyze_team_goals\", analyze_team_goals_submodule)\n",
    "module2_workflow.add_node(\"generate_module2_report_data\", generate_module2_report_data_submodule)\n",
    "module2_workflow.add_node(\"formatter\", formatter_submodule)\n",
    "\n",
    "\n",
    "# 엣지 (실행 순서) 정의\n",
    "# 시작 노드에서 'data_collection' 노드로 연결\n",
    "module2_workflow.add_edge(START, \"data_collection\")\n",
    "# 'data_collection' -> 'calculate_individual_contribution' 순서로 연결\n",
    "module2_workflow.add_edge(\"data_collection\", \"calculate_individual_contribution\")\n",
    "# 'calculate_individual_contribution' -> 'analyze_team_goals' 순서로 연결\n",
    "module2_workflow.add_edge(\"calculate_individual_contribution\", \"analyze_team_goals\")\n",
    "# 'analyze_team_goals' -> 'generate_module2_report_data' 순서로 연결\n",
    "module2_workflow.add_edge(\"analyze_team_goals\", \"generate_module2_report_data\")\n",
    "# 'generate_module2_report_data' -> 'formatter' 순서로 연결\n",
    "module2_workflow.add_edge(\"generate_module2_report_data\", \"formatter\")\n",
    "# 'formatter'에서 최종 종료 지점(END)으로 연결\n",
    "module2_workflow.add_edge(\"formatter\", END)\n",
    "\n",
    "# 모듈 2의 Graph 컴파일\n",
    "module2_graph = module2_workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106f5edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fc9cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "107f6609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "모듈 2 테스트 시작: 2분기 평가\n",
      "============================================================\n",
      "🚀 2분기 평가 실행 중...\n",
      "LLM Call (Cumulative Task Contribution): Task 1 for 김개발\n",
      "LLM Call (Cumulative Task Contribution): Task 2 for 이설계\n",
      "LLM Call (Cumulative Task Contribution): Task 3 for 박DB\n",
      "LLM Call (Cumulative Task Contribution): Task 4 for 김개발\n",
      "LLM Call (Cumulative Task Contribution): Task 5 for 이설계\n",
      "LLM Call (Cumulative Task Contribution): Task 6 for 김개발\n",
      "LLM Call (Cumulative Task Contribution): Task 7 for 이설계\n",
      "LLM Call (Cumulative Task Contribution): Task 8 for 이설계\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# 2분기 평가 실행\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🚀 2분기 평가 실행 중...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m result_q2 = \u001b[43mmodule2_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_q2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ 2분기 평가 완료!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m최종 메시지: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult_q2[\u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m][-\u001b[32m1\u001b[39m].content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2719\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2716\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2717\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2719\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2720\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2723\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2724\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2725\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2728\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2729\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2730\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2731\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2732\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2733\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mints\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINTERRUPT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   2734\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2436\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2434\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2435\u001b[39m             loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2437\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2438\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2439\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2440\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2441\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2442\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2443\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2444\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/langgraph/pregel/runner.py:161\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    159\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/langgraph/pregel/retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/langgraph/utils/runnable.py:623\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    621\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    625\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/langgraph/utils/runnable.py:377\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    375\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 941\u001b[39m, in \u001b[36mcalculate_individual_contribution_submodule\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# 🔧 1번만 호출 (전체 분기 데이터 기반)\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m report_type == \u001b[33m\"\u001b[39m\u001b[33mquarterly\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m941\u001b[39m     llm_results = \u001b[43mcall_llm_for_cumulative_task_contribution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_summary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcumulative_performance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memp_name\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    944\u001b[39m     base_update_data = {\n\u001b[32m    945\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mai_contribution_score\u001b[39m\u001b[33m\"\u001b[39m: llm_results.get(\u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m), \n\u001b[32m    946\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mai_analysis_comment_task\u001b[39m\u001b[33m\"\u001b[39m: llm_results.get(\u001b[33m\"\u001b[39m\u001b[33mcomment\u001b[39m\u001b[33m\"\u001b[39m) \n\u001b[32m    947\u001b[39m     }\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m report_type == \u001b[33m\"\u001b[39m\u001b[33mannual\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 574\u001b[39m, in \u001b[36mcall_llm_for_cumulative_task_contribution\u001b[39m\u001b[34m(task_id, combined_summary, cumulative_performance, target_level, emp_name)\u001b[39m\n\u001b[32m    571\u001b[39m chain = prompt | llm_client\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m     response: AIMessage = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtask_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcombined_summary\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_summary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcumulative_performance\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcumulative_performance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtarget_level\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43memp_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43memp_name\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    581\u001b[39m     json_output_raw = response.content\n\u001b[32m    583\u001b[39m     json_output = _extract_json_from_llm_response(json_output_raw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/langchain_core/runnables/base.py:3047\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3045\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3046\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3047\u001b[39m                 input_ = context.run(step.invoke, input_, config)\n\u001b[32m   3048\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3049\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:372\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> BaseMessage:\n\u001b[32m    369\u001b[39m     config = ensure_config(config)\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    382\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:717\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    715\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m717\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/openai/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/openai/_base_client.py:972\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    970\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m972\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    978\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Caches/pypoetry/virtualenvs/langchain-kr-WTM_qa1c-py3.11/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/lib/python3.11/ssl.py:1295\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1291\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1292\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1293\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1294\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.11/lib/python3.11/ssl.py:1168\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 2분기 및 연말 테스트 실행 예시\n",
    "\n",
    "# 기존 코드는 그대로 두고, 실행 부분만 수정\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"모듈 2 테스트 시작: 2분기 평가\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# === 2분기 평가 실행 ===\n",
    "state_q2 = Module2AgentState(\n",
    "    messages=[HumanMessage(content=\"모듈 2 분기별 평가 시작\")],\n",
    "    report_type=\"quarterly\",\n",
    "    team_id=1,\n",
    "    period_id=2,  # 2024년 2분기 (Q2) 평가\n",
    "    \n",
    "    # Q1, Q2 Task Summaries (id: 1-18 = Q1 9개 + Q2 9개)\n",
    "    target_task_ids=[1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    target_team_kpi_ids=[1, 2, 3, 4],\n",
    "    \n",
    "    team_evaluation_id=102,  # Q2의 team_evaluation_id\n",
    "    \n",
    "    updated_task_ids=None,\n",
    "    updated_team_kpi_ids=None,\n",
    "    kpi_individual_relative_contributions=None\n",
    ")\n",
    "\n",
    "# 2분기 평가 실행\n",
    "print(\"🚀 2분기 평가 실행 중...\")\n",
    "result_q2 = module2_graph.invoke(state_q2)\n",
    "\n",
    "print(\"✅ 2분기 평가 완료!\")\n",
    "print(f\"최종 메시지: {result_q2['messages'][-1].content}\")\n",
    "print(f\"업데이트된 Task 수: {len(result_q2.get('updated_task_ids', []))}\")\n",
    "print(f\"업데이트된 KPI 수: {len(result_q2.get('updated_team_kpi_ids', []))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1986d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53aa3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bfff68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92182de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"모듈 2 테스트 시작: 연말 평가\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "# === 연말 평가 실행 ===\n",
    "state_annual = Module2AgentState(\n",
    "    messages=[HumanMessage(content=\"모듈 2 연말 평가 시작\")],\n",
    "    report_type=\"annual\",\n",
    "    team_id=1,\n",
    "    period_id=4,  # 2024년 연말 (Q4) 평가\n",
    "    \n",
    "    # Q1~Q4 모든 Task Summaries (id: 1-36 = 전체)\n",
    "    target_task_ids=[1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    target_team_kpi_ids=[1, 2, 3, 4],\n",
    "    \n",
    "    team_evaluation_id=104,  # Q4의 team_evaluation_id\n",
    "    \n",
    "    updated_task_ids=None,\n",
    "    updated_team_kpi_ids=None,\n",
    "    kpi_individual_relative_contributions=None\n",
    ")\n",
    "\n",
    "# 연말 평가 실행\n",
    "print(\"🚀 연말 평가 실행 중...\")\n",
    "result_annual = module2_graph.invoke(state_annual)\n",
    "\n",
    "print(\"✅ 연말 평가 완료!\")\n",
    "print(f\"최종 메시지: {result_annual['messages'][-1].content}\")\n",
    "print(f\"업데이트된 Task 수: {len(result_annual.get('updated_task_ids', []))}\")\n",
    "print(f\"업데이트된 KPI 수: {len(result_annual.get('updated_team_kpi_ids', []))}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"결과 확인\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# === 결과 확인 ===\n",
    "def check_results():\n",
    "    \"\"\"결과 확인 함수\"\"\"\n",
    "    with engine.connect() as connection:\n",
    "        print(\"📊 2분기 결과 확인:\")\n",
    "        \n",
    "        # tasks 테이블 확인 (2분기는 기여도만)\n",
    "        print(\"\\n1. Tasks 테이블 AI 결과 (2분기 후):\")\n",
    "        query = text(\"\"\"\n",
    "            SELECT task_id, task_name, emp_no, \n",
    "                   ai_contribution_score, ai_achievement_rate, ai_assessed_grade\n",
    "            FROM tasks \n",
    "            ORDER BY task_id LIMIT 5\n",
    "        \"\"\")\n",
    "        results = connection.execute(query).fetchall()\n",
    "        for row in results:\n",
    "            print(f\"  Task {row.task_id}: 기여도={row.ai_contribution_score}, 달성률={row.ai_achievement_rate}, 등급={row.ai_assessed_grade}\")\n",
    "        \n",
    "        # team_kpis 확인\n",
    "        print(\"\\n2. Team KPIs 결과:\")\n",
    "        query = text(\"\"\"\n",
    "            SELECT kpi_name, ai_kpi_overall_progress_rate, \n",
    "                   LEFT(ai_kpi_analysis_comment, 50) as comment_preview\n",
    "            FROM team_kpis \n",
    "            ORDER BY team_kpi_id\n",
    "        \"\"\")\n",
    "        results = connection.execute(query).fetchall()\n",
    "        for row in results:\n",
    "            print(f\"  {row.kpi_name}: 달성률={row.ai_kpi_overall_progress_rate}%, 코멘트={row.comment_preview}...\")\n",
    "        \n",
    "        # feedback_reports 확인 (2분기)\n",
    "        print(\"\\n3. Feedback Reports (2분기):\")\n",
    "        query = text(\"\"\"\n",
    "            SELECT emp_no, ai_individual_total_contribution_quarterly,\n",
    "                   LEFT(ai_overall_contribution_summary_comment, 50) as comment_preview\n",
    "            FROM feedback_reports \n",
    "            WHERE team_evaluation_id = 102\n",
    "        \"\"\")\n",
    "        results = connection.execute(query).fetchall()\n",
    "        for row in results:\n",
    "            print(f\"  {row.emp_no}: 기여도={row.ai_individual_total_contribution_quarterly}%, 코멘트={row.comment_preview}...\")\n",
    "        \n",
    "        print(\"\\n📊 연말 결과 확인:\")\n",
    "        \n",
    "        # final_evaluation_reports 확인\n",
    "        print(\"\\n4. Final Evaluation Reports (연말):\")\n",
    "        query = text(\"\"\"\n",
    "            SELECT emp_no, ai_annual_individual_total_contribution, ai_annual_achievement_rate,\n",
    "                   LEFT(ai_annual_performance_summary_comment, 50) as comment_preview\n",
    "            FROM final_evaluation_reports \n",
    "            WHERE team_evaluation_id = 104\n",
    "        \"\"\")\n",
    "        results = connection.execute(query).fetchall()\n",
    "        for row in results:\n",
    "            print(f\"  {row.emp_no}: 기여도={row.ai_annual_individual_total_contribution}%, 달성률={row.ai_annual_achievement_rate}%, 코멘트={row.comment_preview}...\")\n",
    "        \n",
    "        # temp_evaluations 확인\n",
    "        print(\"\\n5. Temp Evaluations (중간 평가):\")\n",
    "        query = text(\"\"\"\n",
    "            SELECT TempEvaluation_empNo,\n",
    "                   LEFT(ai_annual_key_performance_contribution_summary, 50) as summary_preview\n",
    "            FROM temp_evaluations \n",
    "            WHERE team_evaluation_id = 104\n",
    "        \"\"\")\n",
    "        results = connection.execute(query).fetchall()\n",
    "        for row in results:\n",
    "            print(f\"  {row.TempEvaluation_empNo}: 요약={row.summary_preview}...\")\n",
    "\n",
    "# 결과 확인 실행\n",
    "check_results()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"모든 테스트 완료! 🎉\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448748de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b69d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c992986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-WTM_qa1c-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
